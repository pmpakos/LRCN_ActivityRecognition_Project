{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import external functions\n",
    "from image_processing import *\n",
    "from utils import *\n",
    "from custom_models import *\n",
    "\n",
    "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 956573 images belonging to 101 classes.\n",
      "Found 471071 images belonging to 101 classes.\n",
      "Found 540949 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load images for training\n",
    "path_to_images = '/data_generator/UCF-101-frames'\n",
    "weight_file_path = 'saved_models/pretrained_model.h5'\n",
    "\n",
    "target_size = (240,320)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_folder = path_to_images + '/train'\n",
    "train_datagen = ImageDataGenerator(rotation_range=10,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   rescale=1./255,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.33)\n",
    "training_batches2 = train_datagen.flow_from_directory(train_folder,\n",
    "                                                     target_size=target_size,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     interpolation='bicubic',\n",
    "                                                     subset='training')\n",
    "training_batches = crop_generator(training_batches2, crop_width, crop_height, random=True)\n",
    "\n",
    "validation_batches2 = train_datagen.flow_from_directory(train_folder,\n",
    "                                                       target_size=target_size,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       interpolation='bicubic',\n",
    "                                                       subset='validation')\n",
    "validation_batches = crop_generator(validation_batches2, crop_width, crop_height, random=True)\n",
    "\n",
    "test_folder = path_to_images + '/test'\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,rescale=1./255)\n",
    "testing_batches2 = test_datagen.flow_from_directory(test_folder,\n",
    "                                                   target_size=target_size,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   interpolation='bicubic')\n",
    "testing_batches = crop_generator(testing_batches2, crop_width, crop_height, random=False)\n",
    "\n",
    "classes_dictionary = {}\n",
    "for cls, idx in training_batches2.class_indices.items():\n",
    "    classes_dictionary[idx] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single frame model definition\n",
    "input_shape = (crop_height,crop_width,3)\n",
    "\n",
    "#SGD Optimizer Parameters\n",
    "base_lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "#Step-based LearningRate Parameters\n",
    "epochs = 1000#120 # 450000\n",
    "gamma = 0.1\n",
    "stepsize = 400#40 # 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained (ILSVRC2012) model weights\n",
    "pm_dict = get_h5_weights(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 227, 227, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 111, 111, 96) 14208       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 55, 55, 96)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm1 (LRN2D)                   (None, 55, 55, 96)   0           pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 55, 55, 48)   0           norm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 55, 55, 48)   0           norm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_0 (Conv2D)                (None, 26, 26, 192)  230592      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 26, 26, 192)  230592      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat2 (Concatenate)           (None, 26, 26, 384)  0           conv2_0[0][0]                    \n",
      "                                                                 conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 13, 13, 384)  0           concat2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm2 (LRN2D)                   (None, 13, 13, 384)  0           pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 13, 13, 512)  1769984     norm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 13, 13, 256)  0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 13, 13, 256)  0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_0 (Conv2D)                (None, 13, 13, 256)  590080      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                (None, 13, 13, 256)  590080      lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat4 (Concatenate)           (None, 13, 13, 512)  0           conv4_0[0][0]                    \n",
      "                                                                 conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 13, 13, 256)  0           concat4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 13, 13, 256)  0           concat4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_0 (Conv2D)                (None, 13, 13, 192)  442560      lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1 (Conv2D)                (None, 13, 13, 192)  442560      lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat5 (Concatenate)           (None, 13, 13, 384)  0           conv5_0[0][0]                    \n",
      "                                                                 conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 6, 6, 384)    0           concat5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 13824)        0           pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Dense)                     (None, 4096)         56627200    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "drop6 (Dropout)                 (None, 4096)         0           fc6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc7 (Dense)                     (None, 4096)         16781312    drop6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop7 (Dropout)                 (None, 4096)         0           fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc8-ucf (Dense)                 (None, 101)          413797      drop7[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 78,132,965\n",
      "Trainable params: 78,132,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CaffeDonahueFunctional(input_shape, num_labels)\n",
    "\n",
    "# Load Pretrained weights into model\n",
    "model = load_pretrained_weights(model, pm_dict)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_but_top(model):\n",
    "    # Used to train just the top layers of the model.\n",
    "    for layer in model.layers[:23]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def freeze_all_but_mid_and_top(model):\n",
    "    # After we fine-tune the dense layers, train deeper.\n",
    "    for layer in model.layers[:1]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[1:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # we need to recompile the model for these modifications to take effect\n",
    "    # we use SGD with a low learning rate 0.001\n",
    "    model = model_compile(model, base_lr, momentum)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, nb_epoch, train_generator, validation_generator , callbacks=[]):\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  steps_per_epoch=100,\n",
    "                                  validation_data=validation_generator,\n",
    "                                  validation_steps=10,\n",
    "                                  epochs=nb_epoch,\n",
    "                                  callbacks=callbacks)\n",
    "\n",
    "    return history,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 5.6903 - acc: 0.0228 - val_loss: 5.2255 - val_acc: 0.0281\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 5.2124 - acc: 0.0422 - val_loss: 5.1325 - val_acc: 0.0594\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 5.0946 - acc: 0.0650 - val_loss: 4.9515 - val_acc: 0.0750\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 5.0081 - acc: 0.0991 - val_loss: 4.9839 - val_acc: 0.0719\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 4.9728 - acc: 0.1091 - val_loss: 5.3462 - val_acc: 0.0906\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 5.0320 - acc: 0.1225 - val_loss: 5.2060 - val_acc: 0.0750\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 5.0541 - acc: 0.1319 - val_loss: 5.2937 - val_acc: 0.0938\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 5.1414 - acc: 0.1378 - val_loss: 5.3406 - val_acc: 0.1250\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 5.1880 - acc: 0.1484 - val_loss: 5.3490 - val_acc: 0.1375\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 5.2889 - acc: 0.1597 - val_loss: 5.3131 - val_acc: 0.1437\n"
     ]
    }
   ],
   "source": [
    "model = freeze_all_but_top(model)\n",
    "history1, model = train_model(model, 10, training_batches, validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP-BASED DECAY\n",
    "# To Keras ston SGD apo mono tou kanei time-based decay\n",
    "# Protimoume Step decay, giati emfanizei kalutero convergence\n",
    "# Isws 8a mporousa na dokimasw kai Adaptive Learning\n",
    "#\n",
    "# Possible solution : https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "# Possible solution : https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "####\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = base_lr\n",
    "    drop = gamma\n",
    "    epochs_drop = stepsize\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 5.1016 - acc: 0.1844 - top_k_categorical_accuracy: 0.4525 - val_loss: 4.9101 - val_acc: 0.2250 - val_top_k_categorical_accuracy: 0.4719\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.91015, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 4.8154 - acc: 0.2191 - top_k_categorical_accuracy: 0.4937 - val_loss: 5.0991 - val_acc: 0.1781 - val_top_k_categorical_accuracy: 0.4375\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.91015\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 4.6793 - acc: 0.2563 - top_k_categorical_accuracy: 0.5353 - val_loss: 5.0130 - val_acc: 0.1844 - val_top_k_categorical_accuracy: 0.4281\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.91015\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 4.6492 - acc: 0.2556 - top_k_categorical_accuracy: 0.5487 - val_loss: 4.8726 - val_acc: 0.2281 - val_top_k_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.91015 to 4.87262, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 4.5710 - acc: 0.2741 - top_k_categorical_accuracy: 0.5656 - val_loss: 4.5779 - val_acc: 0.2687 - val_top_k_categorical_accuracy: 0.5469\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.87262 to 4.57790, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 4.4465 - acc: 0.2859 - top_k_categorical_accuracy: 0.5884 - val_loss: 4.8567 - val_acc: 0.2094 - val_top_k_categorical_accuracy: 0.4625\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.57790\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 4.3961 - acc: 0.2897 - top_k_categorical_accuracy: 0.5984 - val_loss: 4.5810 - val_acc: 0.2656 - val_top_k_categorical_accuracy: 0.5687\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.57790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 4.3450 - acc: 0.3053 - top_k_categorical_accuracy: 0.6128 - val_loss: 4.7117 - val_acc: 0.2687 - val_top_k_categorical_accuracy: 0.5531\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.57790\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 4.1549 - acc: 0.3325 - top_k_categorical_accuracy: 0.6562 - val_loss: 4.5815 - val_acc: 0.2375 - val_top_k_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.57790\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 4.2121 - acc: 0.3162 - top_k_categorical_accuracy: 0.6344 - val_loss: 4.5784 - val_acc: 0.2719 - val_top_k_categorical_accuracy: 0.5531\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.57790\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 4.1469 - acc: 0.3366 - top_k_categorical_accuracy: 0.6484 - val_loss: 4.6846 - val_acc: 0.2375 - val_top_k_categorical_accuracy: 0.5437\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.57790\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 4.0186 - acc: 0.3537 - top_k_categorical_accuracy: 0.6728 - val_loss: 4.8724 - val_acc: 0.2812 - val_top_k_categorical_accuracy: 0.4969\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.57790\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 3.9938 - acc: 0.3684 - top_k_categorical_accuracy: 0.6825 - val_loss: 4.6721 - val_acc: 0.2562 - val_top_k_categorical_accuracy: 0.5469\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.57790\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 3.9576 - acc: 0.3744 - top_k_categorical_accuracy: 0.6766 - val_loss: 4.5280 - val_acc: 0.2875 - val_top_k_categorical_accuracy: 0.5563\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.57790 to 4.52798, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 3.8983 - acc: 0.3659 - top_k_categorical_accuracy: 0.6931 - val_loss: 4.4614 - val_acc: 0.2656 - val_top_k_categorical_accuracy: 0.5437\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.52798 to 4.46144, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 363ms/step - loss: 3.8245 - acc: 0.3859 - top_k_categorical_accuracy: 0.7113 - val_loss: 4.4904 - val_acc: 0.2844 - val_top_k_categorical_accuracy: 0.5719\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 4.46144\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 3.8024 - acc: 0.3931 - top_k_categorical_accuracy: 0.7231 - val_loss: 4.3622 - val_acc: 0.2812 - val_top_k_categorical_accuracy: 0.5969\n",
      "\n",
      "Epoch 00017: val_loss improved from 4.46144 to 4.36216, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 3.7209 - acc: 0.4116 - top_k_categorical_accuracy: 0.7309 - val_loss: 4.4104 - val_acc: 0.3281 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.36216\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 3.7231 - acc: 0.3994 - top_k_categorical_accuracy: 0.7300 - val_loss: 4.1454 - val_acc: 0.3344 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.36216 to 4.14542, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 3.7100 - acc: 0.4125 - top_k_categorical_accuracy: 0.7188 - val_loss: 4.3960 - val_acc: 0.2906 - val_top_k_categorical_accuracy: 0.5781\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.14542\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 3.5550 - acc: 0.4497 - top_k_categorical_accuracy: 0.7534 - val_loss: 4.4869 - val_acc: 0.2562 - val_top_k_categorical_accuracy: 0.5719\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 4.14542\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 3.5430 - acc: 0.4412 - top_k_categorical_accuracy: 0.7578 - val_loss: 4.1624 - val_acc: 0.3406 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.14542\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 3.5119 - acc: 0.4441 - top_k_categorical_accuracy: 0.7628 - val_loss: 4.1723 - val_acc: 0.3344 - val_top_k_categorical_accuracy: 0.6031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_loss did not improve from 4.14542\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 3.4896 - acc: 0.4584 - top_k_categorical_accuracy: 0.7606 - val_loss: 4.3533 - val_acc: 0.3156 - val_top_k_categorical_accuracy: 0.5844\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.14542\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 365ms/step - loss: 3.4232 - acc: 0.4641 - top_k_categorical_accuracy: 0.7694 - val_loss: 4.3324 - val_acc: 0.3125 - val_top_k_categorical_accuracy: 0.6031\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.14542\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 3.4502 - acc: 0.4609 - top_k_categorical_accuracy: 0.7659 - val_loss: 4.1126 - val_acc: 0.3469 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00026: val_loss improved from 4.14542 to 4.11258, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 3.3752 - acc: 0.4816 - top_k_categorical_accuracy: 0.7847 - val_loss: 4.2827 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.11258\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 357ms/step - loss: 3.3389 - acc: 0.4781 - top_k_categorical_accuracy: 0.7966 - val_loss: 4.0494 - val_acc: 0.3625 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00028: val_loss improved from 4.11258 to 4.04944, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 3.3007 - acc: 0.4806 - top_k_categorical_accuracy: 0.7969 - val_loss: 4.0198 - val_acc: 0.3531 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00029: val_loss improved from 4.04944 to 4.01975, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 363ms/step - loss: 3.2843 - acc: 0.5012 - top_k_categorical_accuracy: 0.7828 - val_loss: 3.9597 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00030: val_loss improved from 4.01975 to 3.95970, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 3.2372 - acc: 0.5025 - top_k_categorical_accuracy: 0.8056 - val_loss: 4.1180 - val_acc: 0.3469 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.95970\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 3.1230 - acc: 0.5281 - top_k_categorical_accuracy: 0.8225 - val_loss: 4.2217 - val_acc: 0.3438 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.95970\n",
      "Epoch 33/1000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 3.1779 - acc: 0.5209 - top_k_categorical_accuracy: 0.8141 - val_loss: 4.2495 - val_acc: 0.3250 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.95970\n",
      "Epoch 34/1000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 3.0794 - acc: 0.5456 - top_k_categorical_accuracy: 0.8244 - val_loss: 4.2061 - val_acc: 0.3563 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3.95970\n",
      "Epoch 35/1000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 3.0640 - acc: 0.5428 - top_k_categorical_accuracy: 0.8284 - val_loss: 4.0741 - val_acc: 0.3844 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.95970\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 3.0684 - acc: 0.5397 - top_k_categorical_accuracy: 0.8234 - val_loss: 3.9730 - val_acc: 0.3594 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3.95970\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 2.9847 - acc: 0.5578 - top_k_categorical_accuracy: 0.8319 - val_loss: 4.4965 - val_acc: 0.3344 - val_top_k_categorical_accuracy: 0.5531\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3.95970\n",
      "Epoch 38/1000\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.9225 - acc: 0.5756 - top_k_categorical_accuracy: 0.8472 - val_loss: 3.8873 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.95970 to 3.88728, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 39/1000\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 2.9548 - acc: 0.5716 - top_k_categorical_accuracy: 0.8369 - val_loss: 4.0511 - val_acc: 0.3500 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.88728\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.8773 - acc: 0.5672 - top_k_categorical_accuracy: 0.8619 - val_loss: 3.7399 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00040: val_loss improved from 3.88728 to 3.73986, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 2.8643 - acc: 0.5816 - top_k_categorical_accuracy: 0.8488 - val_loss: 3.9328 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.73986\n",
      "Epoch 42/1000\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.8404 - acc: 0.5769 - top_k_categorical_accuracy: 0.8656 - val_loss: 4.2160 - val_acc: 0.3281 - val_top_k_categorical_accuracy: 0.6062\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3.73986\n",
      "Epoch 43/1000\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.8803 - acc: 0.5569 - top_k_categorical_accuracy: 0.8516 - val_loss: 3.9826 - val_acc: 0.3906 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.73986\n",
      "Epoch 44/1000\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.7792 - acc: 0.6038 - top_k_categorical_accuracy: 0.8703 - val_loss: 3.9922 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.73986\n",
      "Epoch 45/1000\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.7657 - acc: 0.6047 - top_k_categorical_accuracy: 0.8631 - val_loss: 3.9494 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 3.73986\n",
      "Epoch 46/1000\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.7072 - acc: 0.6178 - top_k_categorical_accuracy: 0.8756 - val_loss: 4.1313 - val_acc: 0.3375 - val_top_k_categorical_accuracy: 0.6375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_loss did not improve from 3.73986\n",
      "Epoch 47/1000\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.7101 - acc: 0.6072 - top_k_categorical_accuracy: 0.8691 - val_loss: 3.9518 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 3.73986\n",
      "Epoch 48/1000\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 2.6539 - acc: 0.6203 - top_k_categorical_accuracy: 0.8787 - val_loss: 3.7671 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 3.73986\n",
      "Epoch 49/1000\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 2.6237 - acc: 0.6328 - top_k_categorical_accuracy: 0.8831 - val_loss: 4.0225 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.73986\n",
      "Epoch 50/1000\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.6222 - acc: 0.6266 - top_k_categorical_accuracy: 0.8812 - val_loss: 4.2139 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.73986\n",
      "Epoch 51/1000\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.5540 - acc: 0.6516 - top_k_categorical_accuracy: 0.8906 - val_loss: 4.0416 - val_acc: 0.3656 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 3.73986\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 2.5629 - acc: 0.6400 - top_k_categorical_accuracy: 0.8916 - val_loss: 3.8714 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 3.73986\n",
      "Epoch 53/1000\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.5340 - acc: 0.6491 - top_k_categorical_accuracy: 0.8944 - val_loss: 4.0157 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 3.73986\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.5382 - acc: 0.6444 - top_k_categorical_accuracy: 0.8916 - val_loss: 3.8364 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 3.73986\n",
      "Epoch 55/1000\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.4915 - acc: 0.6488 - top_k_categorical_accuracy: 0.9081 - val_loss: 3.9949 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 3.73986\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.4494 - acc: 0.6672 - top_k_categorical_accuracy: 0.9016 - val_loss: 4.0760 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 3.73986\n",
      "Epoch 57/1000\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.4795 - acc: 0.6587 - top_k_categorical_accuracy: 0.9028 - val_loss: 4.0256 - val_acc: 0.3812 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 3.73986\n",
      "Epoch 58/1000\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.4655 - acc: 0.6591 - top_k_categorical_accuracy: 0.8947 - val_loss: 3.6963 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00058: val_loss improved from 3.73986 to 3.69627, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 59/1000\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 2.4072 - acc: 0.6819 - top_k_categorical_accuracy: 0.9066 - val_loss: 3.9457 - val_acc: 0.3531 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 3.69627\n",
      "Epoch 60/1000\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.3757 - acc: 0.6906 - top_k_categorical_accuracy: 0.9094 - val_loss: 4.1426 - val_acc: 0.3438 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 3.69627\n",
      "Epoch 61/1000\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.3212 - acc: 0.6931 - top_k_categorical_accuracy: 0.9125 - val_loss: 4.1656 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 3.69627\n",
      "Epoch 62/1000\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.3450 - acc: 0.6909 - top_k_categorical_accuracy: 0.9119 - val_loss: 4.0346 - val_acc: 0.3812 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 3.69627\n",
      "Epoch 63/1000\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.3088 - acc: 0.6937 - top_k_categorical_accuracy: 0.9172 - val_loss: 3.9905 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 3.69627\n",
      "Epoch 64/1000\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.2799 - acc: 0.7056 - top_k_categorical_accuracy: 0.9278 - val_loss: 3.9058 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 3.69627\n",
      "Epoch 65/1000\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 2.2847 - acc: 0.7153 - top_k_categorical_accuracy: 0.9150 - val_loss: 3.9021 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 3.69627\n",
      "Epoch 66/1000\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.2103 - acc: 0.7247 - top_k_categorical_accuracy: 0.9272 - val_loss: 3.6867 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00066: val_loss improved from 3.69627 to 3.68672, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 67/1000\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.2491 - acc: 0.7066 - top_k_categorical_accuracy: 0.9241 - val_loss: 4.1489 - val_acc: 0.3656 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 3.68672\n",
      "Epoch 68/1000\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 2.1949 - acc: 0.7228 - top_k_categorical_accuracy: 0.9316 - val_loss: 3.9507 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 3.68672\n",
      "Epoch 69/1000\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.1857 - acc: 0.7316 - top_k_categorical_accuracy: 0.9259 - val_loss: 4.1370 - val_acc: 0.3312 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 3.68672\n",
      "Epoch 70/1000\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.1781 - acc: 0.7369 - top_k_categorical_accuracy: 0.9275 - val_loss: 3.8618 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00070: val_loss did not improve from 3.68672\n",
      "Epoch 71/1000\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.1591 - acc: 0.7341 - top_k_categorical_accuracy: 0.9328 - val_loss: 3.9332 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3.68672\n",
      "Epoch 72/1000\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.1401 - acc: 0.7372 - top_k_categorical_accuracy: 0.9306 - val_loss: 4.0587 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 3.68672\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.1493 - acc: 0.7325 - top_k_categorical_accuracy: 0.9291 - val_loss: 3.9805 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 3.68672\n",
      "Epoch 74/1000\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 2.0774 - acc: 0.7503 - top_k_categorical_accuracy: 0.9441 - val_loss: 4.0999 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 3.68672\n",
      "Epoch 75/1000\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.0654 - acc: 0.7641 - top_k_categorical_accuracy: 0.9434 - val_loss: 4.0372 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 3.68672\n",
      "Epoch 76/1000\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 2.0337 - acc: 0.7622 - top_k_categorical_accuracy: 0.9416 - val_loss: 3.9309 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 3.68672\n",
      "Epoch 77/1000\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.0107 - acc: 0.7669 - top_k_categorical_accuracy: 0.9437 - val_loss: 3.9722 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3.68672\n",
      "Epoch 78/1000\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.0386 - acc: 0.7650 - top_k_categorical_accuracy: 0.9447 - val_loss: 3.9816 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 3.68672\n",
      "Epoch 79/1000\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 2.0710 - acc: 0.7416 - top_k_categorical_accuracy: 0.9422 - val_loss: 4.1069 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 3.68672\n",
      "Epoch 80/1000\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 1.9813 - acc: 0.7766 - top_k_categorical_accuracy: 0.9481 - val_loss: 4.0322 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 3.68672\n",
      "Epoch 81/1000\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.9752 - acc: 0.7738 - top_k_categorical_accuracy: 0.9466 - val_loss: 4.2555 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 3.68672\n",
      "Epoch 82/1000\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 2.0368 - acc: 0.7594 - top_k_categorical_accuracy: 0.9406 - val_loss: 3.8077 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 3.68672\n",
      "Epoch 83/1000\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.9820 - acc: 0.7731 - top_k_categorical_accuracy: 0.9456 - val_loss: 4.0755 - val_acc: 0.3594 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 3.68672\n",
      "Epoch 84/1000\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.9553 - acc: 0.7791 - top_k_categorical_accuracy: 0.9537 - val_loss: 3.9456 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 3.68672\n",
      "Epoch 85/1000\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.9426 - acc: 0.7841 - top_k_categorical_accuracy: 0.9506 - val_loss: 4.2048 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 3.68672\n",
      "Epoch 86/1000\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.8670 - acc: 0.7997 - top_k_categorical_accuracy: 0.9584 - val_loss: 4.0277 - val_acc: 0.3844 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 3.68672\n",
      "Epoch 87/1000\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.9257 - acc: 0.7891 - top_k_categorical_accuracy: 0.9469 - val_loss: 3.9081 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 3.68672\n",
      "Epoch 88/1000\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.8699 - acc: 0.7991 - top_k_categorical_accuracy: 0.9578 - val_loss: 4.1155 - val_acc: 0.3625 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 3.68672\n",
      "Epoch 89/1000\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.8770 - acc: 0.7972 - top_k_categorical_accuracy: 0.9566 - val_loss: 4.3578 - val_acc: 0.3625 - val_top_k_categorical_accuracy: 0.5938\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 3.68672\n",
      "Epoch 90/1000\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.8437 - acc: 0.8012 - top_k_categorical_accuracy: 0.9613 - val_loss: 3.9748 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 3.68672\n",
      "Epoch 91/1000\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.8764 - acc: 0.8009 - top_k_categorical_accuracy: 0.9531 - val_loss: 3.9322 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 3.68672\n",
      "Epoch 92/1000\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 1.8492 - acc: 0.8016 - top_k_categorical_accuracy: 0.9581 - val_loss: 3.9615 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 3.68672\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.8343 - acc: 0.8097 - top_k_categorical_accuracy: 0.9553 - val_loss: 4.2121 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 3.68672\n",
      "Epoch 94/1000\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.8318 - acc: 0.8028 - top_k_categorical_accuracy: 0.9631 - val_loss: 4.2274 - val_acc: 0.3594 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 3.68672\n",
      "Epoch 95/1000\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 352ms/step - loss: 1.7977 - acc: 0.8128 - top_k_categorical_accuracy: 0.9622 - val_loss: 3.8732 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 3.68672\n",
      "Epoch 96/1000\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.7511 - acc: 0.8228 - top_k_categorical_accuracy: 0.9647 - val_loss: 3.8903 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 3.68672\n",
      "Epoch 97/1000\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.7815 - acc: 0.8219 - top_k_categorical_accuracy: 0.9572 - val_loss: 3.6708 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00097: val_loss improved from 3.68672 to 3.67078, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 98/1000\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 362ms/step - loss: 1.7589 - acc: 0.8184 - top_k_categorical_accuracy: 0.9644 - val_loss: 4.1145 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 3.67078\n",
      "Epoch 99/1000\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.7807 - acc: 0.8178 - top_k_categorical_accuracy: 0.9600 - val_loss: 3.9777 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 3.67078\n",
      "Epoch 100/1000\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.7638 - acc: 0.8216 - top_k_categorical_accuracy: 0.9559 - val_loss: 4.0541 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 3.67078\n",
      "Epoch 101/1000\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.7081 - acc: 0.8387 - top_k_categorical_accuracy: 0.9675 - val_loss: 3.7510 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 3.67078\n",
      "Epoch 102/1000\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.7298 - acc: 0.8266 - top_k_categorical_accuracy: 0.9656 - val_loss: 4.1612 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 3.67078\n",
      "Epoch 103/1000\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.7298 - acc: 0.8209 - top_k_categorical_accuracy: 0.9709 - val_loss: 3.8806 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 3.67078\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.7100 - acc: 0.8325 - top_k_categorical_accuracy: 0.9691 - val_loss: 4.3444 - val_acc: 0.3812 - val_top_k_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 3.67078\n",
      "Epoch 105/1000\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.7229 - acc: 0.8247 - top_k_categorical_accuracy: 0.9644 - val_loss: 4.2563 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 3.67078\n",
      "Epoch 106/1000\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.7334 - acc: 0.8234 - top_k_categorical_accuracy: 0.9641 - val_loss: 3.7701 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 3.67078\n",
      "Epoch 107/1000\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.7090 - acc: 0.8316 - top_k_categorical_accuracy: 0.9625 - val_loss: 3.8289 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 3.67078\n",
      "Epoch 108/1000\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.6734 - acc: 0.8456 - top_k_categorical_accuracy: 0.9669 - val_loss: 4.2835 - val_acc: 0.3563 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 3.67078\n",
      "Epoch 109/1000\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.6886 - acc: 0.8328 - top_k_categorical_accuracy: 0.9663 - val_loss: 3.7649 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 3.67078\n",
      "Epoch 110/1000\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.6260 - acc: 0.8469 - top_k_categorical_accuracy: 0.9738 - val_loss: 3.8969 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 3.67078\n",
      "Epoch 111/1000\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.6519 - acc: 0.8438 - top_k_categorical_accuracy: 0.9681 - val_loss: 4.1065 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 3.67078\n",
      "Epoch 112/1000\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.6621 - acc: 0.8359 - top_k_categorical_accuracy: 0.9688 - val_loss: 3.9057 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 3.67078\n",
      "Epoch 113/1000\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 1.6582 - acc: 0.8309 - top_k_categorical_accuracy: 0.9700 - val_loss: 4.1152 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 3.67078\n",
      "Epoch 114/1000\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.6213 - acc: 0.8484 - top_k_categorical_accuracy: 0.9738 - val_loss: 4.1789 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 3.67078\n",
      "Epoch 115/1000\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.6479 - acc: 0.8431 - top_k_categorical_accuracy: 0.9694 - val_loss: 3.9984 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 3.67078\n",
      "Epoch 116/1000\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.5689 - acc: 0.8662 - top_k_categorical_accuracy: 0.9775 - val_loss: 4.3074 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 3.67078\n",
      "Epoch 117/1000\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.5892 - acc: 0.8578 - top_k_categorical_accuracy: 0.9750 - val_loss: 4.0988 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 3.67078\n",
      "Epoch 118/1000\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.5956 - acc: 0.8559 - top_k_categorical_accuracy: 0.9744 - val_loss: 4.1152 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 3.67078\n",
      "Epoch 119/1000\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.5987 - acc: 0.8550 - top_k_categorical_accuracy: 0.9709 - val_loss: 4.2398 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 3.67078\n",
      "Epoch 120/1000\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.5580 - acc: 0.8588 - top_k_categorical_accuracy: 0.9769 - val_loss: 4.1606 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 3.67078\n",
      "Epoch 121/1000\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.5520 - acc: 0.8637 - top_k_categorical_accuracy: 0.9750 - val_loss: 4.0972 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 3.67078\n",
      "Epoch 122/1000\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.5735 - acc: 0.8563 - top_k_categorical_accuracy: 0.9709 - val_loss: 4.0528 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 3.67078\n",
      "Epoch 123/1000\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 1.5550 - acc: 0.8538 - top_k_categorical_accuracy: 0.9766 - val_loss: 4.1660 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 3.67078\n",
      "Epoch 124/1000\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.5585 - acc: 0.8662 - top_k_categorical_accuracy: 0.9716 - val_loss: 4.1885 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 3.67078\n",
      "Epoch 125/1000\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.5350 - acc: 0.8619 - top_k_categorical_accuracy: 0.9756 - val_loss: 4.3747 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 3.67078\n",
      "Epoch 126/1000\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.5330 - acc: 0.8591 - top_k_categorical_accuracy: 0.9772 - val_loss: 3.9969 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 3.67078\n",
      "Epoch 127/1000\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.5240 - acc: 0.8697 - top_k_categorical_accuracy: 0.9747 - val_loss: 3.9338 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 3.67078\n",
      "Epoch 128/1000\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.5020 - acc: 0.8728 - top_k_categorical_accuracy: 0.9778 - val_loss: 3.9861 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 3.67078\n",
      "Epoch 129/1000\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 1.5222 - acc: 0.8681 - top_k_categorical_accuracy: 0.9803 - val_loss: 4.4040 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 3.67078\n",
      "Epoch 130/1000\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.5027 - acc: 0.8725 - top_k_categorical_accuracy: 0.9781 - val_loss: 4.1935 - val_acc: 0.3750 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 3.67078\n",
      "Epoch 131/1000\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4807 - acc: 0.8853 - top_k_categorical_accuracy: 0.9800 - val_loss: 4.1865 - val_acc: 0.3719 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 3.67078\n",
      "Epoch 132/1000\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4743 - acc: 0.8775 - top_k_categorical_accuracy: 0.9809 - val_loss: 3.6676 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00132: val_loss improved from 3.67078 to 3.66764, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 133/1000\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 1.4864 - acc: 0.8800 - top_k_categorical_accuracy: 0.9825 - val_loss: 3.9599 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 3.66764\n",
      "Epoch 134/1000\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.5080 - acc: 0.8694 - top_k_categorical_accuracy: 0.9750 - val_loss: 3.9814 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 3.66764\n",
      "Epoch 135/1000\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4636 - acc: 0.8838 - top_k_categorical_accuracy: 0.9788 - val_loss: 4.1187 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 3.66764\n",
      "Epoch 136/1000\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4536 - acc: 0.8878 - top_k_categorical_accuracy: 0.9775 - val_loss: 4.0285 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 3.66764\n",
      "Epoch 137/1000\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4719 - acc: 0.8716 - top_k_categorical_accuracy: 0.9791 - val_loss: 4.0788 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 3.66764\n",
      "Epoch 138/1000\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4656 - acc: 0.8775 - top_k_categorical_accuracy: 0.9791 - val_loss: 3.9662 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 3.66764\n",
      "Epoch 139/1000\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4727 - acc: 0.8794 - top_k_categorical_accuracy: 0.9791 - val_loss: 3.9657 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 3.66764\n",
      "Epoch 140/1000\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4543 - acc: 0.8847 - top_k_categorical_accuracy: 0.9822 - val_loss: 3.9986 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 3.66764\n",
      "Epoch 141/1000\n",
      "\n",
      "Epoch 00141: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4440 - acc: 0.8891 - top_k_categorical_accuracy: 0.9806 - val_loss: 4.1889 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 3.66764\n",
      "Epoch 142/1000\n",
      "\n",
      "Epoch 00142: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4184 - acc: 0.8916 - top_k_categorical_accuracy: 0.9797 - val_loss: 3.9410 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 3.66764\n",
      "Epoch 143/1000\n",
      "\n",
      "Epoch 00143: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 351ms/step - loss: 1.4231 - acc: 0.8906 - top_k_categorical_accuracy: 0.9825 - val_loss: 4.1501 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 3.66764\n",
      "Epoch 144/1000\n",
      "\n",
      "Epoch 00144: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.4077 - acc: 0.8925 - top_k_categorical_accuracy: 0.9819 - val_loss: 4.1019 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 3.66764\n",
      "Epoch 145/1000\n",
      "\n",
      "Epoch 00145: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.4328 - acc: 0.8847 - top_k_categorical_accuracy: 0.9834 - val_loss: 3.7885 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 3.66764\n",
      "Epoch 146/1000\n",
      "\n",
      "Epoch 00146: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.4155 - acc: 0.8884 - top_k_categorical_accuracy: 0.9819 - val_loss: 3.9816 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 3.66764\n",
      "Epoch 147/1000\n",
      "\n",
      "Epoch 00147: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3899 - acc: 0.8941 - top_k_categorical_accuracy: 0.9828 - val_loss: 3.9249 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 3.66764\n",
      "Epoch 148/1000\n",
      "\n",
      "Epoch 00148: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.4004 - acc: 0.8878 - top_k_categorical_accuracy: 0.9859 - val_loss: 3.9458 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 3.66764\n",
      "Epoch 149/1000\n",
      "\n",
      "Epoch 00149: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.4179 - acc: 0.8834 - top_k_categorical_accuracy: 0.9816 - val_loss: 3.9481 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 3.66764\n",
      "Epoch 150/1000\n",
      "\n",
      "Epoch 00150: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.3812 - acc: 0.8937 - top_k_categorical_accuracy: 0.9859 - val_loss: 3.6946 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 3.66764\n",
      "Epoch 151/1000\n",
      "\n",
      "Epoch 00151: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 1.3711 - acc: 0.8984 - top_k_categorical_accuracy: 0.9844 - val_loss: 3.7776 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 3.66764\n",
      "Epoch 152/1000\n",
      "\n",
      "Epoch 00152: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 1.4053 - acc: 0.8856 - top_k_categorical_accuracy: 0.9828 - val_loss: 3.8456 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 3.66764\n",
      "Epoch 153/1000\n",
      "\n",
      "Epoch 00153: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 1.3695 - acc: 0.8975 - top_k_categorical_accuracy: 0.9844 - val_loss: 4.1122 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 3.66764\n",
      "Epoch 154/1000\n",
      "\n",
      "Epoch 00154: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3884 - acc: 0.8944 - top_k_categorical_accuracy: 0.9841 - val_loss: 4.0276 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 3.66764\n",
      "Epoch 155/1000\n",
      "\n",
      "Epoch 00155: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.4009 - acc: 0.8909 - top_k_categorical_accuracy: 0.9803 - val_loss: 3.8503 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 3.66764\n",
      "Epoch 156/1000\n",
      "\n",
      "Epoch 00156: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.3325 - acc: 0.9041 - top_k_categorical_accuracy: 0.9894 - val_loss: 3.9044 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 3.66764\n",
      "Epoch 157/1000\n",
      "\n",
      "Epoch 00157: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.3281 - acc: 0.9119 - top_k_categorical_accuracy: 0.9853 - val_loss: 4.1395 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 3.66764\n",
      "Epoch 158/1000\n",
      "\n",
      "Epoch 00158: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3452 - acc: 0.8994 - top_k_categorical_accuracy: 0.9859 - val_loss: 4.3312 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 3.66764\n",
      "Epoch 159/1000\n",
      "\n",
      "Epoch 00159: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.3575 - acc: 0.8953 - top_k_categorical_accuracy: 0.9856 - val_loss: 4.1946 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 3.66764\n",
      "Epoch 160/1000\n",
      "\n",
      "Epoch 00160: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3577 - acc: 0.9006 - top_k_categorical_accuracy: 0.9856 - val_loss: 4.3746 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 3.66764\n",
      "Epoch 161/1000\n",
      "\n",
      "Epoch 00161: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3513 - acc: 0.9000 - top_k_categorical_accuracy: 0.9838 - val_loss: 4.0572 - val_acc: 0.3844 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 3.66764\n",
      "Epoch 162/1000\n",
      "\n",
      "Epoch 00162: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.3212 - acc: 0.9094 - top_k_categorical_accuracy: 0.9853 - val_loss: 4.2991 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6094\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 3.66764\n",
      "Epoch 163/1000\n",
      "\n",
      "Epoch 00163: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3341 - acc: 0.9069 - top_k_categorical_accuracy: 0.9847 - val_loss: 4.1619 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 3.66764\n",
      "Epoch 164/1000\n",
      "\n",
      "Epoch 00164: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.3409 - acc: 0.8916 - top_k_categorical_accuracy: 0.9887 - val_loss: 3.9335 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 3.66764\n",
      "Epoch 165/1000\n",
      "\n",
      "Epoch 00165: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.3089 - acc: 0.9053 - top_k_categorical_accuracy: 0.9884 - val_loss: 3.9992 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 3.66764\n",
      "Epoch 166/1000\n",
      "\n",
      "Epoch 00166: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3535 - acc: 0.8984 - top_k_categorical_accuracy: 0.9841 - val_loss: 4.3623 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 3.66764\n",
      "Epoch 167/1000\n",
      "\n",
      "Epoch 00167: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.3229 - acc: 0.9006 - top_k_categorical_accuracy: 0.9894 - val_loss: 3.8874 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 3.66764\n",
      "Epoch 168/1000\n",
      "\n",
      "Epoch 00168: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2869 - acc: 0.9116 - top_k_categorical_accuracy: 0.9916 - val_loss: 3.6590 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7438\n",
      "\n",
      "Epoch 00168: val_loss improved from 3.66764 to 3.65903, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 169/1000\n",
      "\n",
      "Epoch 00169: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 1.3091 - acc: 0.9006 - top_k_categorical_accuracy: 0.9869 - val_loss: 3.8520 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 3.65903\n",
      "Epoch 170/1000\n",
      "\n",
      "Epoch 00170: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.3294 - acc: 0.9009 - top_k_categorical_accuracy: 0.9825 - val_loss: 4.0994 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 3.65903\n",
      "Epoch 171/1000\n",
      "\n",
      "Epoch 00171: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2847 - acc: 0.9144 - top_k_categorical_accuracy: 0.9894 - val_loss: 3.7036 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 3.65903\n",
      "Epoch 172/1000\n",
      "\n",
      "Epoch 00172: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2910 - acc: 0.9128 - top_k_categorical_accuracy: 0.9866 - val_loss: 3.9602 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 3.65903\n",
      "Epoch 173/1000\n",
      "\n",
      "Epoch 00173: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2919 - acc: 0.9181 - top_k_categorical_accuracy: 0.9875 - val_loss: 3.9924 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 3.65903\n",
      "Epoch 174/1000\n",
      "\n",
      "Epoch 00174: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2864 - acc: 0.9147 - top_k_categorical_accuracy: 0.9875 - val_loss: 3.8732 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 3.65903\n",
      "Epoch 175/1000\n",
      "\n",
      "Epoch 00175: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2679 - acc: 0.9200 - top_k_categorical_accuracy: 0.9897 - val_loss: 4.1734 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 3.65903\n",
      "Epoch 176/1000\n",
      "\n",
      "Epoch 00176: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2736 - acc: 0.9166 - top_k_categorical_accuracy: 0.9881 - val_loss: 4.2804 - val_acc: 0.3812 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 3.65903\n",
      "Epoch 177/1000\n",
      "\n",
      "Epoch 00177: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.2831 - acc: 0.9081 - top_k_categorical_accuracy: 0.9872 - val_loss: 3.6413 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00177: val_loss improved from 3.65903 to 3.64130, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 178/1000\n",
      "\n",
      "Epoch 00178: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 1.2519 - acc: 0.9181 - top_k_categorical_accuracy: 0.9894 - val_loss: 4.1492 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 3.64130\n",
      "Epoch 179/1000\n",
      "\n",
      "Epoch 00179: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2565 - acc: 0.9197 - top_k_categorical_accuracy: 0.9906 - val_loss: 3.8908 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 3.64130\n",
      "Epoch 180/1000\n",
      "\n",
      "Epoch 00180: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2648 - acc: 0.9206 - top_k_categorical_accuracy: 0.9869 - val_loss: 4.4254 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 3.64130\n",
      "Epoch 181/1000\n",
      "\n",
      "Epoch 00181: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2548 - acc: 0.9188 - top_k_categorical_accuracy: 0.9891 - val_loss: 4.0286 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 3.64130\n",
      "Epoch 182/1000\n",
      "\n",
      "Epoch 00182: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2757 - acc: 0.9106 - top_k_categorical_accuracy: 0.9897 - val_loss: 3.8669 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 3.64130\n",
      "Epoch 183/1000\n",
      "\n",
      "Epoch 00183: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.2567 - acc: 0.9119 - top_k_categorical_accuracy: 0.9903 - val_loss: 3.8534 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 3.64130\n",
      "Epoch 184/1000\n",
      "\n",
      "Epoch 00184: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2689 - acc: 0.9128 - top_k_categorical_accuracy: 0.9869 - val_loss: 4.1333 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 3.64130\n",
      "Epoch 185/1000\n",
      "\n",
      "Epoch 00185: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.2355 - acc: 0.9200 - top_k_categorical_accuracy: 0.9909 - val_loss: 4.1222 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 3.64130\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 00186: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2548 - acc: 0.9137 - top_k_categorical_accuracy: 0.9887 - val_loss: 4.0568 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 3.64130\n",
      "Epoch 187/1000\n",
      "\n",
      "Epoch 00187: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 1.2296 - acc: 0.9194 - top_k_categorical_accuracy: 0.9897 - val_loss: 4.3091 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 3.64130\n",
      "Epoch 188/1000\n",
      "\n",
      "Epoch 00188: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.2334 - acc: 0.9228 - top_k_categorical_accuracy: 0.9925 - val_loss: 3.6540 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 3.64130\n",
      "Epoch 189/1000\n",
      "\n",
      "Epoch 00189: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2444 - acc: 0.9141 - top_k_categorical_accuracy: 0.9887 - val_loss: 4.0239 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 3.64130\n",
      "Epoch 190/1000\n",
      "\n",
      "Epoch 00190: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2276 - acc: 0.9213 - top_k_categorical_accuracy: 0.9903 - val_loss: 4.2661 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 3.64130\n",
      "Epoch 191/1000\n",
      "\n",
      "Epoch 00191: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2275 - acc: 0.9159 - top_k_categorical_accuracy: 0.9894 - val_loss: 3.9928 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 3.64130\n",
      "Epoch 192/1000\n",
      "\n",
      "Epoch 00192: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2415 - acc: 0.9178 - top_k_categorical_accuracy: 0.9891 - val_loss: 4.3223 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 3.64130\n",
      "Epoch 193/1000\n",
      "\n",
      "Epoch 00193: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2150 - acc: 0.9300 - top_k_categorical_accuracy: 0.9891 - val_loss: 4.4399 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 3.64130\n",
      "Epoch 194/1000\n",
      "\n",
      "Epoch 00194: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.2039 - acc: 0.9287 - top_k_categorical_accuracy: 0.9916 - val_loss: 4.9708 - val_acc: 0.3656 - val_top_k_categorical_accuracy: 0.5719\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 3.64130\n",
      "Epoch 195/1000\n",
      "\n",
      "Epoch 00195: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2183 - acc: 0.9262 - top_k_categorical_accuracy: 0.9875 - val_loss: 4.0155 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 3.64130\n",
      "Epoch 196/1000\n",
      "\n",
      "Epoch 00196: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1862 - acc: 0.9325 - top_k_categorical_accuracy: 0.9925 - val_loss: 4.3408 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 3.64130\n",
      "Epoch 197/1000\n",
      "\n",
      "Epoch 00197: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1991 - acc: 0.9272 - top_k_categorical_accuracy: 0.9909 - val_loss: 4.0422 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 3.64130\n",
      "Epoch 198/1000\n",
      "\n",
      "Epoch 00198: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 1.1883 - acc: 0.9284 - top_k_categorical_accuracy: 0.9925 - val_loss: 3.9899 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 3.64130\n",
      "Epoch 199/1000\n",
      "\n",
      "Epoch 00199: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2000 - acc: 0.9313 - top_k_categorical_accuracy: 0.9891 - val_loss: 4.1621 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 3.64130\n",
      "Epoch 200/1000\n",
      "\n",
      "Epoch 00200: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1922 - acc: 0.9291 - top_k_categorical_accuracy: 0.9912 - val_loss: 3.9924 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 3.64130\n",
      "Epoch 201/1000\n",
      "\n",
      "Epoch 00201: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2041 - acc: 0.9256 - top_k_categorical_accuracy: 0.9912 - val_loss: 3.9387 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 3.64130\n",
      "Epoch 202/1000\n",
      "\n",
      "Epoch 00202: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.1973 - acc: 0.9234 - top_k_categorical_accuracy: 0.9922 - val_loss: 3.9610 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 3.64130\n",
      "Epoch 203/1000\n",
      "\n",
      "Epoch 00203: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2111 - acc: 0.9216 - top_k_categorical_accuracy: 0.9906 - val_loss: 4.3733 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 3.64130\n",
      "Epoch 204/1000\n",
      "\n",
      "Epoch 00204: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.1802 - acc: 0.9319 - top_k_categorical_accuracy: 0.9906 - val_loss: 4.1385 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 3.64130\n",
      "Epoch 205/1000\n",
      "\n",
      "Epoch 00205: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.2018 - acc: 0.9272 - top_k_categorical_accuracy: 0.9900 - val_loss: 3.8265 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 3.64130\n",
      "Epoch 206/1000\n",
      "\n",
      "Epoch 00206: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1472 - acc: 0.9366 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.0637 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 3.64130\n",
      "Epoch 207/1000\n",
      "\n",
      "Epoch 00207: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1844 - acc: 0.9284 - top_k_categorical_accuracy: 0.9906 - val_loss: 3.9588 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 3.64130\n",
      "Epoch 208/1000\n",
      "\n",
      "Epoch 00208: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1902 - acc: 0.9256 - top_k_categorical_accuracy: 0.9894 - val_loss: 4.1507 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 3.64130\n",
      "Epoch 209/1000\n",
      "\n",
      "Epoch 00209: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 1.1430 - acc: 0.9344 - top_k_categorical_accuracy: 0.9931 - val_loss: 4.0685 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 3.64130\n",
      "Epoch 210/1000\n",
      "\n",
      "Epoch 00210: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.2007 - acc: 0.9247 - top_k_categorical_accuracy: 0.9878 - val_loss: 4.0443 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 3.64130\n",
      "Epoch 211/1000\n",
      "\n",
      "Epoch 00211: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1795 - acc: 0.9262 - top_k_categorical_accuracy: 0.9919 - val_loss: 3.8514 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 3.64130\n",
      "Epoch 212/1000\n",
      "\n",
      "Epoch 00212: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1983 - acc: 0.9225 - top_k_categorical_accuracy: 0.9878 - val_loss: 3.6208 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00212: val_loss improved from 3.64130 to 3.62084, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 213/1000\n",
      "\n",
      "Epoch 00213: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 1.1579 - acc: 0.9341 - top_k_categorical_accuracy: 0.9922 - val_loss: 3.7160 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 3.62084\n",
      "Epoch 214/1000\n",
      "\n",
      "Epoch 00214: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1819 - acc: 0.9237 - top_k_categorical_accuracy: 0.9916 - val_loss: 3.7553 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 3.62084\n",
      "Epoch 215/1000\n",
      "\n",
      "Epoch 00215: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1685 - acc: 0.9344 - top_k_categorical_accuracy: 0.9919 - val_loss: 4.2268 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 3.62084\n",
      "Epoch 216/1000\n",
      "\n",
      "Epoch 00216: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1283 - acc: 0.9400 - top_k_categorical_accuracy: 0.9928 - val_loss: 4.2363 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 3.62084\n",
      "Epoch 217/1000\n",
      "\n",
      "Epoch 00217: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1567 - acc: 0.9300 - top_k_categorical_accuracy: 0.9928 - val_loss: 4.2859 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 3.62084\n",
      "Epoch 218/1000\n",
      "\n",
      "Epoch 00218: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 1.1466 - acc: 0.9338 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.1572 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 3.62084\n",
      "Epoch 219/1000\n",
      "\n",
      "Epoch 00219: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1218 - acc: 0.9391 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.0964 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 3.62084\n",
      "Epoch 220/1000\n",
      "\n",
      "Epoch 00220: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1829 - acc: 0.9259 - top_k_categorical_accuracy: 0.9912 - val_loss: 4.3542 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 3.62084\n",
      "Epoch 221/1000\n",
      "\n",
      "Epoch 00221: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 1.1445 - acc: 0.9353 - top_k_categorical_accuracy: 0.9941 - val_loss: 4.3045 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 3.62084\n",
      "Epoch 222/1000\n",
      "\n",
      "Epoch 00222: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1512 - acc: 0.9303 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.0135 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 3.62084\n",
      "Epoch 223/1000\n",
      "\n",
      "Epoch 00223: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1415 - acc: 0.9322 - top_k_categorical_accuracy: 0.9947 - val_loss: 4.2688 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 3.62084\n",
      "Epoch 224/1000\n",
      "\n",
      "Epoch 00224: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.1270 - acc: 0.9400 - top_k_categorical_accuracy: 0.9925 - val_loss: 4.1950 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 3.62084\n",
      "Epoch 225/1000\n",
      "\n",
      "Epoch 00225: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1198 - acc: 0.9412 - top_k_categorical_accuracy: 0.9931 - val_loss: 3.9858 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 3.62084\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 00226: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1243 - acc: 0.9350 - top_k_categorical_accuracy: 0.9941 - val_loss: 4.2281 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 3.62084\n",
      "Epoch 227/1000\n",
      "\n",
      "Epoch 00227: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1306 - acc: 0.9372 - top_k_categorical_accuracy: 0.9941 - val_loss: 4.0831 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 3.62084\n",
      "Epoch 228/1000\n",
      "\n",
      "Epoch 00228: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 1.1231 - acc: 0.9400 - top_k_categorical_accuracy: 0.9912 - val_loss: 4.0386 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 3.62084\n",
      "Epoch 229/1000\n",
      "\n",
      "Epoch 00229: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1364 - acc: 0.9322 - top_k_categorical_accuracy: 0.9916 - val_loss: 4.1101 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 3.62084\n",
      "Epoch 230/1000\n",
      "\n",
      "Epoch 00230: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1549 - acc: 0.9356 - top_k_categorical_accuracy: 0.9900 - val_loss: 3.6812 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 3.62084\n",
      "Epoch 231/1000\n",
      "\n",
      "Epoch 00231: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1296 - acc: 0.9356 - top_k_categorical_accuracy: 0.9931 - val_loss: 3.3722 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7469\n",
      "\n",
      "Epoch 00231: val_loss improved from 3.62084 to 3.37218, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 232/1000\n",
      "\n",
      "Epoch 00232: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1377 - acc: 0.9334 - top_k_categorical_accuracy: 0.9897 - val_loss: 4.0931 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 3.37218\n",
      "Epoch 233/1000\n",
      "\n",
      "Epoch 00233: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1214 - acc: 0.9363 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.2620 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 3.37218\n",
      "Epoch 234/1000\n",
      "\n",
      "Epoch 00234: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1130 - acc: 0.9400 - top_k_categorical_accuracy: 0.9953 - val_loss: 3.7820 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 3.37218\n",
      "Epoch 235/1000\n",
      "\n",
      "Epoch 00235: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.1168 - acc: 0.9391 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.1142 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 3.37218\n",
      "Epoch 236/1000\n",
      "\n",
      "Epoch 00236: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0902 - acc: 0.9447 - top_k_categorical_accuracy: 0.9925 - val_loss: 3.7447 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7438\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 3.37218\n",
      "Epoch 237/1000\n",
      "\n",
      "Epoch 00237: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 1.1156 - acc: 0.9369 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.0673 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 3.37218\n",
      "Epoch 238/1000\n",
      "\n",
      "Epoch 00238: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0955 - acc: 0.9416 - top_k_categorical_accuracy: 0.9941 - val_loss: 3.8810 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 3.37218\n",
      "Epoch 239/1000\n",
      "\n",
      "Epoch 00239: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1076 - acc: 0.9412 - top_k_categorical_accuracy: 0.9922 - val_loss: 3.7187 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 3.37218\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00240: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 1.0947 - acc: 0.9456 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.5652 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 3.37218\n",
      "Epoch 241/1000\n",
      "\n",
      "Epoch 00241: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1137 - acc: 0.9378 - top_k_categorical_accuracy: 0.9919 - val_loss: 4.2281 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 3.37218\n",
      "Epoch 242/1000\n",
      "\n",
      "Epoch 00242: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0849 - acc: 0.9437 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.3007 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 3.37218\n",
      "Epoch 243/1000\n",
      "\n",
      "Epoch 00243: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0997 - acc: 0.9431 - top_k_categorical_accuracy: 0.9916 - val_loss: 4.0846 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 3.37218\n",
      "Epoch 244/1000\n",
      "\n",
      "Epoch 00244: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0877 - acc: 0.9472 - top_k_categorical_accuracy: 0.9922 - val_loss: 4.1995 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 3.37218\n",
      "Epoch 245/1000\n",
      "\n",
      "Epoch 00245: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0910 - acc: 0.9397 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.2870 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 3.37218\n",
      "Epoch 246/1000\n",
      "\n",
      "Epoch 00246: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0829 - acc: 0.9441 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.1161 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 3.37218\n",
      "Epoch 247/1000\n",
      "\n",
      "Epoch 00247: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.1026 - acc: 0.9375 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.1061 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 3.37218\n",
      "Epoch 248/1000\n",
      "\n",
      "Epoch 00248: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0940 - acc: 0.9409 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.2438 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 3.37218\n",
      "Epoch 249/1000\n",
      "\n",
      "Epoch 00249: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0615 - acc: 0.9484 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.1803 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 3.37218\n",
      "Epoch 250/1000\n",
      "\n",
      "Epoch 00250: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0862 - acc: 0.9472 - top_k_categorical_accuracy: 0.9916 - val_loss: 4.0565 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 3.37218\n",
      "Epoch 251/1000\n",
      "\n",
      "Epoch 00251: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0638 - acc: 0.9484 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.2880 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 3.37218\n",
      "Epoch 252/1000\n",
      "\n",
      "Epoch 00252: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0724 - acc: 0.9463 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.1051 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 3.37218\n",
      "Epoch 253/1000\n",
      "\n",
      "Epoch 00253: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0727 - acc: 0.9469 - top_k_categorical_accuracy: 0.9919 - val_loss: 4.1578 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 3.37218\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00254: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 362ms/step - loss: 1.0844 - acc: 0.9434 - top_k_categorical_accuracy: 0.9931 - val_loss: 4.0598 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 3.37218\n",
      "Epoch 255/1000\n",
      "\n",
      "Epoch 00255: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0993 - acc: 0.9375 - top_k_categorical_accuracy: 0.9919 - val_loss: 4.2011 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 3.37218\n",
      "Epoch 256/1000\n",
      "\n",
      "Epoch 00256: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0775 - acc: 0.9434 - top_k_categorical_accuracy: 0.9925 - val_loss: 3.9799 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 3.37218\n",
      "Epoch 257/1000\n",
      "\n",
      "Epoch 00257: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0892 - acc: 0.9372 - top_k_categorical_accuracy: 0.9931 - val_loss: 4.3398 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 3.37218\n",
      "Epoch 258/1000\n",
      "\n",
      "Epoch 00258: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0404 - acc: 0.9522 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.1270 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 3.37218\n",
      "Epoch 259/1000\n",
      "\n",
      "Epoch 00259: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0485 - acc: 0.9434 - top_k_categorical_accuracy: 0.9963 - val_loss: 4.2422 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 3.37218\n",
      "Epoch 260/1000\n",
      "\n",
      "Epoch 00260: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0531 - acc: 0.9469 - top_k_categorical_accuracy: 0.9922 - val_loss: 3.6887 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 3.37218\n",
      "Epoch 261/1000\n",
      "\n",
      "Epoch 00261: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0652 - acc: 0.9419 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.1616 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 3.37218\n",
      "Epoch 262/1000\n",
      "\n",
      "Epoch 00262: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0384 - acc: 0.9525 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.3023 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 3.37218\n",
      "Epoch 263/1000\n",
      "\n",
      "Epoch 00263: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0717 - acc: 0.9403 - top_k_categorical_accuracy: 0.9944 - val_loss: 3.7428 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7375\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 3.37218\n",
      "Epoch 264/1000\n",
      "\n",
      "Epoch 00264: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 1.0621 - acc: 0.9463 - top_k_categorical_accuracy: 0.9922 - val_loss: 4.5330 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6031\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 3.37218\n",
      "Epoch 265/1000\n",
      "\n",
      "Epoch 00265: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0600 - acc: 0.9459 - top_k_categorical_accuracy: 0.9922 - val_loss: 4.1008 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 3.37218\n",
      "Epoch 266/1000\n",
      "\n",
      "Epoch 00266: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0674 - acc: 0.9403 - top_k_categorical_accuracy: 0.9938 - val_loss: 3.8096 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 3.37218\n",
      "Epoch 267/1000\n",
      "\n",
      "Epoch 00267: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0539 - acc: 0.9425 - top_k_categorical_accuracy: 0.9950 - val_loss: 4.1161 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 3.37218\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00268: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0490 - acc: 0.9500 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.5267 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 3.37218\n",
      "Epoch 269/1000\n",
      "\n",
      "Epoch 00269: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0326 - acc: 0.9534 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.6489 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 3.37218\n",
      "Epoch 270/1000\n",
      "\n",
      "Epoch 00270: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0449 - acc: 0.9503 - top_k_categorical_accuracy: 0.9934 - val_loss: 4.2018 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 3.37218\n",
      "Epoch 271/1000\n",
      "\n",
      "Epoch 00271: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0268 - acc: 0.9519 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.0142 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 3.37218\n",
      "Epoch 272/1000\n",
      "\n",
      "Epoch 00272: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0385 - acc: 0.9503 - top_k_categorical_accuracy: 0.9950 - val_loss: 4.3997 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 3.37218\n",
      "Epoch 273/1000\n",
      "\n",
      "Epoch 00273: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0628 - acc: 0.9459 - top_k_categorical_accuracy: 0.9903 - val_loss: 4.0363 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 3.37218\n",
      "Epoch 274/1000\n",
      "\n",
      "Epoch 00274: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0125 - acc: 0.9547 - top_k_categorical_accuracy: 0.9959 - val_loss: 3.7907 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 3.37218\n",
      "Epoch 275/1000\n",
      "\n",
      "Epoch 00275: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 1.0169 - acc: 0.9544 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.0512 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 3.37218\n",
      "Epoch 276/1000\n",
      "\n",
      "Epoch 00276: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0280 - acc: 0.9513 - top_k_categorical_accuracy: 0.9941 - val_loss: 3.9068 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 3.37218\n",
      "Epoch 277/1000\n",
      "\n",
      "Epoch 00277: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0349 - acc: 0.9466 - top_k_categorical_accuracy: 0.9956 - val_loss: 3.6966 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 3.37218\n",
      "Epoch 278/1000\n",
      "\n",
      "Epoch 00278: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0386 - acc: 0.9472 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.0019 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 3.37218\n",
      "Epoch 279/1000\n",
      "\n",
      "Epoch 00279: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0269 - acc: 0.9503 - top_k_categorical_accuracy: 0.9947 - val_loss: 3.9537 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 3.37218\n",
      "Epoch 280/1000\n",
      "\n",
      "Epoch 00280: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0136 - acc: 0.9541 - top_k_categorical_accuracy: 0.9956 - val_loss: 3.8479 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 3.37218\n",
      "Epoch 281/1000\n",
      "\n",
      "Epoch 00281: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0156 - acc: 0.9531 - top_k_categorical_accuracy: 0.9953 - val_loss: 3.9263 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 3.37218\n",
      "Epoch 282/1000\n",
      "\n",
      "Epoch 00282: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0120 - acc: 0.9528 - top_k_categorical_accuracy: 0.9956 - val_loss: 3.8453 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 3.37218\n",
      "Epoch 283/1000\n",
      "\n",
      "Epoch 00283: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0189 - acc: 0.9488 - top_k_categorical_accuracy: 0.9966 - val_loss: 3.9416 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 3.37218\n",
      "Epoch 284/1000\n",
      "\n",
      "Epoch 00284: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0505 - acc: 0.9425 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.2406 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 3.37218\n",
      "Epoch 285/1000\n",
      "\n",
      "Epoch 00285: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 1.0453 - acc: 0.9409 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.3697 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6094\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 3.37218\n",
      "Epoch 286/1000\n",
      "\n",
      "Epoch 00286: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 1.0495 - acc: 0.9437 - top_k_categorical_accuracy: 0.9931 - val_loss: 4.5349 - val_acc: 0.3625 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 3.37218\n",
      "Epoch 287/1000\n",
      "\n",
      "Epoch 00287: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0247 - acc: 0.9478 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.5937 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 3.37218\n",
      "Epoch 288/1000\n",
      "\n",
      "Epoch 00288: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.0133 - acc: 0.9559 - top_k_categorical_accuracy: 0.9938 - val_loss: 4.2349 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 3.37218\n",
      "Epoch 289/1000\n",
      "\n",
      "Epoch 00289: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 1.0322 - acc: 0.9462 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.1230 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 3.37218\n",
      "Epoch 290/1000\n",
      "\n",
      "Epoch 00290: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 1.0069 - acc: 0.9488 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.0071 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 3.37218\n",
      "Epoch 291/1000\n",
      "\n",
      "Epoch 00291: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 1.0126 - acc: 0.9528 - top_k_categorical_accuracy: 0.9941 - val_loss: 4.5408 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 3.37218\n",
      "Epoch 292/1000\n",
      "\n",
      "Epoch 00292: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9943 - acc: 0.9566 - top_k_categorical_accuracy: 0.9950 - val_loss: 4.2322 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 3.37218\n",
      "Epoch 293/1000\n",
      "\n",
      "Epoch 00293: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9852 - acc: 0.9609 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9340 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 3.37218\n",
      "Epoch 294/1000\n",
      "\n",
      "Epoch 00294: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9892 - acc: 0.9581 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9359 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 3.37218\n",
      "Epoch 295/1000\n",
      "\n",
      "Epoch 00295: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 1.0003 - acc: 0.9506 - top_k_categorical_accuracy: 0.9975 - val_loss: 4.3586 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 3.37218\n",
      "Epoch 296/1000\n",
      "\n",
      "Epoch 00296: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9878 - acc: 0.9544 - top_k_categorical_accuracy: 0.9969 - val_loss: 3.8968 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 3.37218\n",
      "Epoch 297/1000\n",
      "\n",
      "Epoch 00297: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.9893 - acc: 0.9522 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.3433 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 3.37218\n",
      "Epoch 298/1000\n",
      "\n",
      "Epoch 00298: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 1.0101 - acc: 0.9503 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.0723 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 3.37218\n",
      "Epoch 299/1000\n",
      "\n",
      "Epoch 00299: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 1.0079 - acc: 0.9519 - top_k_categorical_accuracy: 0.9947 - val_loss: 4.2520 - val_acc: 0.3844 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 3.37218\n",
      "Epoch 300/1000\n",
      "\n",
      "Epoch 00300: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.9783 - acc: 0.9559 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.7964 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 3.37218\n",
      "Epoch 301/1000\n",
      "\n",
      "Epoch 00301: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9963 - acc: 0.9525 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.4059 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 3.37218\n",
      "Epoch 302/1000\n",
      "\n",
      "Epoch 00302: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9913 - acc: 0.9556 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.1239 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 3.37218\n",
      "Epoch 303/1000\n",
      "\n",
      "Epoch 00303: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9861 - acc: 0.9569 - top_k_categorical_accuracy: 0.9956 - val_loss: 3.8357 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 3.37218\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 00304: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.9962 - acc: 0.9528 - top_k_categorical_accuracy: 0.9947 - val_loss: 4.1023 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 3.37218\n",
      "Epoch 305/1000\n",
      "\n",
      "Epoch 00305: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9862 - acc: 0.9550 - top_k_categorical_accuracy: 0.9941 - val_loss: 4.2526 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 3.37218\n",
      "Epoch 306/1000\n",
      "\n",
      "Epoch 00306: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9847 - acc: 0.9559 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.0300 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 3.37218\n",
      "Epoch 307/1000\n",
      "\n",
      "Epoch 00307: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9809 - acc: 0.9572 - top_k_categorical_accuracy: 0.9975 - val_loss: 4.2863 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 3.37218\n",
      "Epoch 308/1000\n",
      "\n",
      "Epoch 00308: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9926 - acc: 0.9531 - top_k_categorical_accuracy: 0.9950 - val_loss: 3.5417 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 3.37218\n",
      "Epoch 309/1000\n",
      "\n",
      "Epoch 00309: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9922 - acc: 0.9519 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.5067 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 3.37218\n",
      "Epoch 310/1000\n",
      "\n",
      "Epoch 00310: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9864 - acc: 0.9559 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.0659 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 3.37218\n",
      "Epoch 311/1000\n",
      "\n",
      "Epoch 00311: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.9489 - acc: 0.9625 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.5406 - val_acc: 0.3594 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 3.37218\n",
      "Epoch 312/1000\n",
      "\n",
      "Epoch 00312: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9817 - acc: 0.9516 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.5726 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 3.37218\n",
      "Epoch 313/1000\n",
      "\n",
      "Epoch 00313: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9904 - acc: 0.9566 - top_k_categorical_accuracy: 0.9928 - val_loss: 4.1326 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6094\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 3.37218\n",
      "Epoch 314/1000\n",
      "\n",
      "Epoch 00314: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9573 - acc: 0.9647 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9395 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 3.37218\n",
      "Epoch 315/1000\n",
      "\n",
      "Epoch 00315: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9634 - acc: 0.9613 - top_k_categorical_accuracy: 0.9944 - val_loss: 4.3097 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 3.37218\n",
      "Epoch 316/1000\n",
      "\n",
      "Epoch 00316: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9790 - acc: 0.9513 - top_k_categorical_accuracy: 0.9953 - val_loss: 3.8274 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 3.37218\n",
      "Epoch 317/1000\n",
      "\n",
      "Epoch 00317: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9732 - acc: 0.9569 - top_k_categorical_accuracy: 0.9934 - val_loss: 3.8372 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 3.37218\n",
      "Epoch 318/1000\n",
      "\n",
      "Epoch 00318: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9598 - acc: 0.9594 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.1792 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 3.37218\n",
      "Epoch 319/1000\n",
      "\n",
      "Epoch 00319: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9720 - acc: 0.9566 - top_k_categorical_accuracy: 0.9966 - val_loss: 3.8779 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 3.37218\n",
      "Epoch 320/1000\n",
      "\n",
      "Epoch 00320: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9848 - acc: 0.9500 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.1617 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 3.37218\n",
      "Epoch 321/1000\n",
      "\n",
      "Epoch 00321: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9672 - acc: 0.9575 - top_k_categorical_accuracy: 0.9950 - val_loss: 4.2842 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 3.37218\n",
      "Epoch 322/1000\n",
      "\n",
      "Epoch 00322: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.9530 - acc: 0.9572 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.2622 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 3.37218\n",
      "Epoch 323/1000\n",
      "\n",
      "Epoch 00323: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9816 - acc: 0.9525 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.0593 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 3.37218\n",
      "Epoch 324/1000\n",
      "\n",
      "Epoch 00324: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9486 - acc: 0.9569 - top_k_categorical_accuracy: 0.9975 - val_loss: 4.0404 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 3.37218\n",
      "Epoch 325/1000\n",
      "\n",
      "Epoch 00325: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9531 - acc: 0.9572 - top_k_categorical_accuracy: 0.9966 - val_loss: 3.7841 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 3.37218\n",
      "Epoch 326/1000\n",
      "\n",
      "Epoch 00326: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9585 - acc: 0.9616 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.7413 - val_acc: 0.3906 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 3.37218\n",
      "Epoch 327/1000\n",
      "\n",
      "Epoch 00327: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9574 - acc: 0.9572 - top_k_categorical_accuracy: 0.9947 - val_loss: 4.3943 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 3.37218\n",
      "Epoch 328/1000\n",
      "\n",
      "Epoch 00328: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9275 - acc: 0.9691 - top_k_categorical_accuracy: 0.9950 - val_loss: 3.7928 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 3.37218\n",
      "Epoch 329/1000\n",
      "\n",
      "Epoch 00329: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.9796 - acc: 0.9500 - top_k_categorical_accuracy: 0.9947 - val_loss: 3.9141 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 3.37218\n",
      "Epoch 330/1000\n",
      "\n",
      "Epoch 00330: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9765 - acc: 0.9519 - top_k_categorical_accuracy: 0.9947 - val_loss: 4.0478 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 3.37218\n",
      "Epoch 331/1000\n",
      "\n",
      "Epoch 00331: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9614 - acc: 0.9578 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.4105 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 3.37218\n",
      "Epoch 332/1000\n",
      "\n",
      "Epoch 00332: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9710 - acc: 0.9513 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.1808 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 3.37218\n",
      "Epoch 333/1000\n",
      "\n",
      "Epoch 00333: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9267 - acc: 0.9669 - top_k_categorical_accuracy: 0.9975 - val_loss: 3.7704 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 3.37218\n",
      "Epoch 334/1000\n",
      "\n",
      "Epoch 00334: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9359 - acc: 0.9647 - top_k_categorical_accuracy: 0.9947 - val_loss: 3.9696 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 3.37218\n",
      "Epoch 335/1000\n",
      "\n",
      "Epoch 00335: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9383 - acc: 0.9597 - top_k_categorical_accuracy: 0.9969 - val_loss: 3.8157 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 3.37218\n",
      "Epoch 336/1000\n",
      "\n",
      "Epoch 00336: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9569 - acc: 0.9537 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.0111 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 3.37218\n",
      "Epoch 337/1000\n",
      "\n",
      "Epoch 00337: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9619 - acc: 0.9547 - top_k_categorical_accuracy: 0.9947 - val_loss: 3.6586 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 3.37218\n",
      "Epoch 338/1000\n",
      "\n",
      "Epoch 00338: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9493 - acc: 0.9547 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.6963 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 3.37218\n",
      "Epoch 339/1000\n",
      "\n",
      "Epoch 00339: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.9354 - acc: 0.9631 - top_k_categorical_accuracy: 0.9950 - val_loss: 4.3778 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 3.37218\n",
      "Epoch 340/1000\n",
      "\n",
      "Epoch 00340: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9270 - acc: 0.9644 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.6815 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 3.37218\n",
      "Epoch 341/1000\n",
      "\n",
      "Epoch 00341: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9273 - acc: 0.9625 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.1864 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 3.37218\n",
      "Epoch 342/1000\n",
      "\n",
      "Epoch 00342: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9361 - acc: 0.9578 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.8139 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 3.37218\n",
      "Epoch 343/1000\n",
      "\n",
      "Epoch 00343: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9134 - acc: 0.9653 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.7463 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 3.37218\n",
      "Epoch 344/1000\n",
      "\n",
      "Epoch 00344: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9326 - acc: 0.9606 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.3622 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 3.37218\n",
      "Epoch 345/1000\n",
      "\n",
      "Epoch 00345: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9206 - acc: 0.9625 - top_k_categorical_accuracy: 0.9975 - val_loss: 3.8726 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 3.37218\n",
      "Epoch 346/1000\n",
      "\n",
      "Epoch 00346: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9372 - acc: 0.9600 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.4021 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 3.37218\n",
      "Epoch 347/1000\n",
      "\n",
      "Epoch 00347: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9263 - acc: 0.9653 - top_k_categorical_accuracy: 0.9963 - val_loss: 4.6585 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 3.37218\n",
      "Epoch 348/1000\n",
      "\n",
      "Epoch 00348: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9402 - acc: 0.9562 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9477 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 3.37218\n",
      "Epoch 349/1000\n",
      "\n",
      "Epoch 00349: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9298 - acc: 0.9594 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.3301 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 3.37218\n",
      "Epoch 350/1000\n",
      "\n",
      "Epoch 00350: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9311 - acc: 0.9587 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.0200 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 3.37218\n",
      "Epoch 351/1000\n",
      "\n",
      "Epoch 00351: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9365 - acc: 0.9575 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.4532 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 3.37218\n",
      "Epoch 352/1000\n",
      "\n",
      "Epoch 00352: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9424 - acc: 0.9534 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.2297 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 3.37218\n",
      "Epoch 353/1000\n",
      "\n",
      "Epoch 00353: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9210 - acc: 0.9613 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.2403 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 3.37218\n",
      "Epoch 354/1000\n",
      "\n",
      "Epoch 00354: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9396 - acc: 0.9569 - top_k_categorical_accuracy: 0.9938 - val_loss: 3.9425 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 3.37218\n",
      "Epoch 355/1000\n",
      "\n",
      "Epoch 00355: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9182 - acc: 0.9650 - top_k_categorical_accuracy: 0.9959 - val_loss: 3.9392 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 3.37218\n",
      "Epoch 356/1000\n",
      "\n",
      "Epoch 00356: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9284 - acc: 0.9609 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9695 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 3.37218\n",
      "Epoch 357/1000\n",
      "\n",
      "Epoch 00357: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9144 - acc: 0.9591 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.1013 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 3.37218\n",
      "Epoch 358/1000\n",
      "\n",
      "Epoch 00358: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9421 - acc: 0.9531 - top_k_categorical_accuracy: 0.9953 - val_loss: 3.9002 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 3.37218\n",
      "Epoch 359/1000\n",
      "\n",
      "Epoch 00359: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9171 - acc: 0.9575 - top_k_categorical_accuracy: 0.9975 - val_loss: 4.2129 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 3.37218\n",
      "Epoch 360/1000\n",
      "\n",
      "Epoch 00360: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9026 - acc: 0.9647 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.9167 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 3.37218\n",
      "Epoch 361/1000\n",
      "\n",
      "Epoch 00361: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9114 - acc: 0.9613 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.2323 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 3.37218\n",
      "Epoch 362/1000\n",
      "\n",
      "Epoch 00362: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9148 - acc: 0.9584 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.0970 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 3.37218\n",
      "Epoch 363/1000\n",
      "\n",
      "Epoch 00363: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9170 - acc: 0.9622 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.2814 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 3.37218\n",
      "Epoch 364/1000\n",
      "\n",
      "Epoch 00364: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9131 - acc: 0.9634 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.0306 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 3.37218\n",
      "Epoch 365/1000\n",
      "\n",
      "Epoch 00365: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9163 - acc: 0.9609 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.8376 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 3.37218\n",
      "Epoch 366/1000\n",
      "\n",
      "Epoch 00366: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.9096 - acc: 0.9641 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.5238 - val_acc: 0.3656 - val_top_k_categorical_accuracy: 0.5750\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 3.37218\n",
      "Epoch 367/1000\n",
      "\n",
      "Epoch 00367: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8937 - acc: 0.9688 - top_k_categorical_accuracy: 0.9966 - val_loss: 3.9547 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 3.37218\n",
      "Epoch 368/1000\n",
      "\n",
      "Epoch 00368: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.9071 - acc: 0.9628 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.2619 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 3.37218\n",
      "Epoch 369/1000\n",
      "\n",
      "Epoch 00369: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.9185 - acc: 0.9578 - top_k_categorical_accuracy: 0.9963 - val_loss: 4.3483 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 3.37218\n",
      "Epoch 370/1000\n",
      "\n",
      "Epoch 00370: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9149 - acc: 0.9600 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.3646 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 3.37218\n",
      "Epoch 371/1000\n",
      "\n",
      "Epoch 00371: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.9042 - acc: 0.9603 - top_k_categorical_accuracy: 0.9969 - val_loss: 3.7522 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 3.37218\n",
      "Epoch 372/1000\n",
      "\n",
      "Epoch 00372: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.8993 - acc: 0.9616 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7814 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 3.37218\n",
      "Epoch 373/1000\n",
      "\n",
      "Epoch 00373: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.9034 - acc: 0.9653 - top_k_categorical_accuracy: 0.9953 - val_loss: 4.2080 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 3.37218\n",
      "Epoch 374/1000\n",
      "\n",
      "Epoch 00374: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.9144 - acc: 0.9575 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.2293 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 3.37218\n",
      "Epoch 375/1000\n",
      "\n",
      "Epoch 00375: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8958 - acc: 0.9631 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.5527 - val_acc: 0.3969 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 3.37218\n",
      "Epoch 376/1000\n",
      "\n",
      "Epoch 00376: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.8905 - acc: 0.9706 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.1103 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 3.37218\n",
      "Epoch 377/1000\n",
      "\n",
      "Epoch 00377: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.9130 - acc: 0.9591 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.5523 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6062\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 3.37218\n",
      "Epoch 378/1000\n",
      "\n",
      "Epoch 00378: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8840 - acc: 0.9691 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.1215 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 3.37218\n",
      "Epoch 379/1000\n",
      "\n",
      "Epoch 00379: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8832 - acc: 0.9697 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.4384 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 3.37218\n",
      "Epoch 380/1000\n",
      "\n",
      "Epoch 00380: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8866 - acc: 0.9666 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.1393 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 3.37218\n",
      "Epoch 381/1000\n",
      "\n",
      "Epoch 00381: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.8977 - acc: 0.9609 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.1481 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 3.37218\n",
      "Epoch 382/1000\n",
      "\n",
      "Epoch 00382: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8964 - acc: 0.9647 - top_k_categorical_accuracy: 0.9963 - val_loss: 4.2014 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 3.37218\n",
      "Epoch 383/1000\n",
      "\n",
      "Epoch 00383: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8890 - acc: 0.9647 - top_k_categorical_accuracy: 0.9956 - val_loss: 4.1996 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 3.37218\n",
      "Epoch 384/1000\n",
      "\n",
      "Epoch 00384: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8914 - acc: 0.9606 - top_k_categorical_accuracy: 0.9966 - val_loss: 3.7856 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 3.37218\n",
      "Epoch 385/1000\n",
      "\n",
      "Epoch 00385: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8906 - acc: 0.9666 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.0963 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 3.37218\n",
      "Epoch 386/1000\n",
      "\n",
      "Epoch 00386: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8971 - acc: 0.9609 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1970 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 3.37218\n",
      "Epoch 387/1000\n",
      "\n",
      "Epoch 00387: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8851 - acc: 0.9628 - top_k_categorical_accuracy: 0.9959 - val_loss: 4.0120 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 3.37218\n",
      "Epoch 388/1000\n",
      "\n",
      "Epoch 00388: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8819 - acc: 0.9647 - top_k_categorical_accuracy: 0.9963 - val_loss: 4.0846 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 3.37218\n",
      "Epoch 389/1000\n",
      "\n",
      "Epoch 00389: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.8896 - acc: 0.9631 - top_k_categorical_accuracy: 0.9950 - val_loss: 3.6726 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 3.37218\n",
      "Epoch 390/1000\n",
      "\n",
      "Epoch 00390: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8812 - acc: 0.9678 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.8395 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 3.37218\n",
      "Epoch 391/1000\n",
      "\n",
      "Epoch 00391: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8809 - acc: 0.9641 - top_k_categorical_accuracy: 0.9959 - val_loss: 3.8872 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 3.37218\n",
      "Epoch 392/1000\n",
      "\n",
      "Epoch 00392: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8906 - acc: 0.9644 - top_k_categorical_accuracy: 0.9966 - val_loss: 4.1001 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 3.37218\n",
      "Epoch 393/1000\n",
      "\n",
      "Epoch 00393: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8699 - acc: 0.9728 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.5333 - val_acc: 0.4000 - val_top_k_categorical_accuracy: 0.6094\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 3.37218\n",
      "Epoch 394/1000\n",
      "\n",
      "Epoch 00394: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8581 - acc: 0.9700 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.1115 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 3.37218\n",
      "Epoch 395/1000\n",
      "\n",
      "Epoch 00395: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8643 - acc: 0.9709 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.9552 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 3.37218\n",
      "Epoch 396/1000\n",
      "\n",
      "Epoch 00396: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8770 - acc: 0.9672 - top_k_categorical_accuracy: 0.9969 - val_loss: 4.5006 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 3.37218\n",
      "Epoch 397/1000\n",
      "\n",
      "Epoch 00397: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8593 - acc: 0.9712 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.8485 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 3.37218\n",
      "Epoch 398/1000\n",
      "\n",
      "Epoch 00398: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8642 - acc: 0.9691 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.6781 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 3.37218\n",
      "Epoch 399/1000\n",
      "\n",
      "Epoch 00399: LearningRateScheduler setting learning rate to 0.001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8768 - acc: 0.9650 - top_k_categorical_accuracy: 0.9959 - val_loss: 3.9642 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 3.37218\n",
      "Epoch 400/1000\n",
      "\n",
      "Epoch 00400: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8665 - acc: 0.9703 - top_k_categorical_accuracy: 0.9953 - val_loss: 3.6061 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 3.37218\n",
      "Epoch 401/1000\n",
      "\n",
      "Epoch 00401: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8334 - acc: 0.9756 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7592 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 3.37218\n",
      "Epoch 402/1000\n",
      "\n",
      "Epoch 00402: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8360 - acc: 0.9738 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9249 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 3.37218\n",
      "Epoch 403/1000\n",
      "\n",
      "Epoch 00403: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.8313 - acc: 0.9778 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.8765 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 3.37218\n",
      "Epoch 404/1000\n",
      "\n",
      "Epoch 00404: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8245 - acc: 0.9828 - top_k_categorical_accuracy: 0.9975 - val_loss: 3.8572 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 3.37218\n",
      "Epoch 405/1000\n",
      "\n",
      "Epoch 00405: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8079 - acc: 0.9831 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.2513 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 3.37218\n",
      "Epoch 406/1000\n",
      "\n",
      "Epoch 00406: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8125 - acc: 0.9838 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4092 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 3.37218\n",
      "Epoch 407/1000\n",
      "\n",
      "Epoch 00407: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8167 - acc: 0.9816 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1311 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 3.37218\n",
      "Epoch 408/1000\n",
      "\n",
      "Epoch 00408: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8130 - acc: 0.9828 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8509 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 3.37218\n",
      "Epoch 409/1000\n",
      "\n",
      "Epoch 00409: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8088 - acc: 0.9850 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.1930 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 3.37218\n",
      "Epoch 410/1000\n",
      "\n",
      "Epoch 00410: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8066 - acc: 0.9875 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9277 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 3.37218\n",
      "Epoch 411/1000\n",
      "\n",
      "Epoch 00411: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8036 - acc: 0.9869 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8958 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 3.37218\n",
      "Epoch 412/1000\n",
      "\n",
      "Epoch 00412: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8061 - acc: 0.9856 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0786 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 3.37218\n",
      "Epoch 413/1000\n",
      "\n",
      "Epoch 00413: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8043 - acc: 0.9853 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9149 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 3.37218\n",
      "Epoch 414/1000\n",
      "\n",
      "Epoch 00414: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8067 - acc: 0.9828 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9857 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 3.37218\n",
      "Epoch 415/1000\n",
      "\n",
      "Epoch 00415: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8122 - acc: 0.9825 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.1778 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 3.37218\n",
      "Epoch 416/1000\n",
      "\n",
      "Epoch 00416: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.8011 - acc: 0.9850 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4310 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7406\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 3.37218\n",
      "Epoch 417/1000\n",
      "\n",
      "Epoch 00417: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8082 - acc: 0.9847 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6372 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 3.37218\n",
      "Epoch 418/1000\n",
      "\n",
      "Epoch 00418: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8200 - acc: 0.9797 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.4021 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7438\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 3.37218\n",
      "Epoch 419/1000\n",
      "\n",
      "Epoch 00419: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8003 - acc: 0.9866 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.9620 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 3.37218\n",
      "Epoch 420/1000\n",
      "\n",
      "Epoch 00420: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8120 - acc: 0.9822 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.6755 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 3.37218\n",
      "Epoch 421/1000\n",
      "\n",
      "Epoch 00421: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.8052 - acc: 0.9856 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.0446 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 3.37218\n",
      "Epoch 422/1000\n",
      "\n",
      "Epoch 00422: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8034 - acc: 0.9853 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9410 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 3.37218\n",
      "Epoch 423/1000\n",
      "\n",
      "Epoch 00423: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8029 - acc: 0.9856 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7952 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 3.37218\n",
      "Epoch 424/1000\n",
      "\n",
      "Epoch 00424: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7937 - acc: 0.9878 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9994 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 3.37218\n",
      "Epoch 425/1000\n",
      "\n",
      "Epoch 00425: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8021 - acc: 0.9853 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6422 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 3.37218\n",
      "Epoch 426/1000\n",
      "\n",
      "Epoch 00426: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7974 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1775 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 3.37218\n",
      "Epoch 427/1000\n",
      "\n",
      "Epoch 00427: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7993 - acc: 0.9869 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7436 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 3.37218\n",
      "Epoch 428/1000\n",
      "\n",
      "Epoch 00428: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7988 - acc: 0.9853 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0516 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 3.37218\n",
      "Epoch 429/1000\n",
      "\n",
      "Epoch 00429: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8059 - acc: 0.9825 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9387 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 3.37218\n",
      "Epoch 430/1000\n",
      "\n",
      "Epoch 00430: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8034 - acc: 0.9825 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6516 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 3.37218\n",
      "Epoch 431/1000\n",
      "\n",
      "Epoch 00431: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7892 - acc: 0.9847 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.3796 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 3.37218\n",
      "Epoch 432/1000\n",
      "\n",
      "Epoch 00432: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8032 - acc: 0.9834 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1025 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 3.37218\n",
      "Epoch 433/1000\n",
      "\n",
      "Epoch 00433: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7940 - acc: 0.9881 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.1221 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 3.37218\n",
      "Epoch 434/1000\n",
      "\n",
      "Epoch 00434: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8035 - acc: 0.9834 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9757 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 3.37218\n",
      "Epoch 435/1000\n",
      "\n",
      "Epoch 00435: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7876 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9934 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 3.37218\n",
      "Epoch 436/1000\n",
      "\n",
      "Epoch 00436: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7903 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8060 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 3.37218\n",
      "Epoch 437/1000\n",
      "\n",
      "Epoch 00437: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.8036 - acc: 0.9841 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8379 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 3.37218\n",
      "Epoch 438/1000\n",
      "\n",
      "Epoch 00438: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8026 - acc: 0.9828 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7969 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 3.37218\n",
      "Epoch 439/1000\n",
      "\n",
      "Epoch 00439: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7926 - acc: 0.9872 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9430 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 3.37218\n",
      "Epoch 440/1000\n",
      "\n",
      "Epoch 00440: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7930 - acc: 0.9847 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6990 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 3.37218\n",
      "Epoch 441/1000\n",
      "\n",
      "Epoch 00441: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7953 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9941 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 3.37218\n",
      "Epoch 442/1000\n",
      "\n",
      "Epoch 00442: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7976 - acc: 0.9859 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8846 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 3.37218\n",
      "Epoch 443/1000\n",
      "\n",
      "Epoch 00443: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7992 - acc: 0.9838 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4615 - val_acc: 0.5500 - val_top_k_categorical_accuracy: 0.7562\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 3.37218\n",
      "Epoch 444/1000\n",
      "\n",
      "Epoch 00444: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7993 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.0572 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 3.37218\n",
      "Epoch 445/1000\n",
      "\n",
      "Epoch 00445: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7906 - acc: 0.9881 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1046 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 3.37218\n",
      "Epoch 446/1000\n",
      "\n",
      "Epoch 00446: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.8015 - acc: 0.9853 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.9255 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 3.37218\n",
      "Epoch 447/1000\n",
      "\n",
      "Epoch 00447: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7933 - acc: 0.9856 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9648 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 3.37218\n",
      "Epoch 448/1000\n",
      "\n",
      "Epoch 00448: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.8000 - acc: 0.9834 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0496 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 3.37218\n",
      "Epoch 449/1000\n",
      "\n",
      "Epoch 00449: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7885 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0677 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 3.37218\n",
      "Epoch 450/1000\n",
      "\n",
      "Epoch 00450: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7952 - acc: 0.9844 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9039 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 3.37218\n",
      "Epoch 451/1000\n",
      "\n",
      "Epoch 00451: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7926 - acc: 0.9872 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.5141 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 3.37218\n",
      "Epoch 452/1000\n",
      "\n",
      "Epoch 00452: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7935 - acc: 0.9850 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0795 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 3.37218\n",
      "Epoch 453/1000\n",
      "\n",
      "Epoch 00453: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7901 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2012 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 3.37218\n",
      "Epoch 454/1000\n",
      "\n",
      "Epoch 00454: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7854 - acc: 0.9878 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.1585 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 3.37218\n",
      "Epoch 455/1000\n",
      "\n",
      "Epoch 00455: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7876 - acc: 0.9866 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0723 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 3.37218\n",
      "Epoch 456/1000\n",
      "\n",
      "Epoch 00456: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7833 - acc: 0.9887 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9630 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 3.37218\n",
      "Epoch 457/1000\n",
      "\n",
      "Epoch 00457: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7802 - acc: 0.9891 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7841 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 3.37218\n",
      "Epoch 458/1000\n",
      "\n",
      "Epoch 00458: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7890 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8456 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 3.37218\n",
      "Epoch 459/1000\n",
      "\n",
      "Epoch 00459: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.8006 - acc: 0.9822 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8228 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 3.37218\n",
      "Epoch 460/1000\n",
      "\n",
      "Epoch 00460: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7947 - acc: 0.9844 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6680 - val_acc: 0.5250 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 3.37218\n",
      "Epoch 461/1000\n",
      "\n",
      "Epoch 00461: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7959 - acc: 0.9853 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.1557 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 3.37218\n",
      "Epoch 462/1000\n",
      "\n",
      "Epoch 00462: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7904 - acc: 0.9878 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.2279 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 3.37218\n",
      "Epoch 463/1000\n",
      "\n",
      "Epoch 00463: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7922 - acc: 0.9838 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6674 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 3.37218\n",
      "Epoch 464/1000\n",
      "\n",
      "Epoch 00464: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7901 - acc: 0.9859 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.2269 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 3.37218\n",
      "Epoch 465/1000\n",
      "\n",
      "Epoch 00465: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7895 - acc: 0.9866 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.0936 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 3.37218\n",
      "Epoch 466/1000\n",
      "\n",
      "Epoch 00466: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7942 - acc: 0.9850 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.9600 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 3.37218\n",
      "Epoch 467/1000\n",
      "\n",
      "Epoch 00467: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7874 - acc: 0.9878 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0570 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 3.37218\n",
      "Epoch 468/1000\n",
      "\n",
      "Epoch 00468: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7907 - acc: 0.9856 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.5173 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 3.37218\n",
      "Epoch 469/1000\n",
      "\n",
      "Epoch 00469: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7932 - acc: 0.9844 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.9700 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 3.37218\n",
      "Epoch 470/1000\n",
      "\n",
      "Epoch 00470: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7788 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8053 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 3.37218\n",
      "Epoch 471/1000\n",
      "\n",
      "Epoch 00471: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7838 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8421 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 3.37218\n",
      "Epoch 472/1000\n",
      "\n",
      "Epoch 00472: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7816 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1091 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 3.37218\n",
      "Epoch 473/1000\n",
      "\n",
      "Epoch 00473: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7789 - acc: 0.9881 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7978 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 3.37218\n",
      "Epoch 474/1000\n",
      "\n",
      "Epoch 00474: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7946 - acc: 0.9844 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8784 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 3.37218\n",
      "Epoch 475/1000\n",
      "\n",
      "Epoch 00475: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7846 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0600 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 3.37218\n",
      "Epoch 476/1000\n",
      "\n",
      "Epoch 00476: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7901 - acc: 0.9856 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.4389 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 3.37218\n",
      "Epoch 477/1000\n",
      "\n",
      "Epoch 00477: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7855 - acc: 0.9866 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0538 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 3.37218\n",
      "Epoch 478/1000\n",
      "\n",
      "Epoch 00478: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7897 - acc: 0.9838 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7264 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 3.37218\n",
      "Epoch 479/1000\n",
      "\n",
      "Epoch 00479: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7782 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1932 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 3.37218\n",
      "Epoch 480/1000\n",
      "\n",
      "Epoch 00480: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7788 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6775 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 3.37218\n",
      "Epoch 481/1000\n",
      "\n",
      "Epoch 00481: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7843 - acc: 0.9878 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9567 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 3.37218\n",
      "Epoch 482/1000\n",
      "\n",
      "Epoch 00482: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7766 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9917 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 3.37218\n",
      "Epoch 483/1000\n",
      "\n",
      "Epoch 00483: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7970 - acc: 0.9844 - top_k_categorical_accuracy: 0.9975 - val_loss: 4.1943 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 3.37218\n",
      "Epoch 484/1000\n",
      "\n",
      "Epoch 00484: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7791 - acc: 0.9884 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2165 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 3.37218\n",
      "Epoch 485/1000\n",
      "\n",
      "Epoch 00485: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7762 - acc: 0.9878 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9418 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 3.37218\n",
      "Epoch 486/1000\n",
      "\n",
      "Epoch 00486: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7740 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8205 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 3.37218\n",
      "Epoch 487/1000\n",
      "\n",
      "Epoch 00487: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7864 - acc: 0.9875 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7925 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 3.37218\n",
      "Epoch 488/1000\n",
      "\n",
      "Epoch 00488: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7720 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0981 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 3.37218\n",
      "Epoch 489/1000\n",
      "\n",
      "Epoch 00489: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7697 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7184 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 3.37218\n",
      "Epoch 490/1000\n",
      "\n",
      "Epoch 00490: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7758 - acc: 0.9878 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.4623 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7406\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 3.37218\n",
      "Epoch 491/1000\n",
      "\n",
      "Epoch 00491: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7814 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6355 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 3.37218\n",
      "Epoch 492/1000\n",
      "\n",
      "Epoch 00492: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7865 - acc: 0.9825 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9062 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 3.37218\n",
      "Epoch 493/1000\n",
      "\n",
      "Epoch 00493: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7751 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0835 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 3.37218\n",
      "Epoch 494/1000\n",
      "\n",
      "Epoch 00494: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7771 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9483 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 3.37218\n",
      "Epoch 495/1000\n",
      "\n",
      "Epoch 00495: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7746 - acc: 0.9878 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9356 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 3.37218\n",
      "Epoch 496/1000\n",
      "\n",
      "Epoch 00496: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7862 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7619 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 3.37218\n",
      "Epoch 497/1000\n",
      "\n",
      "Epoch 00497: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7754 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9043 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 3.37218\n",
      "Epoch 498/1000\n",
      "\n",
      "Epoch 00498: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7835 - acc: 0.9847 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.2588 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 3.37218\n",
      "Epoch 499/1000\n",
      "\n",
      "Epoch 00499: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7841 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.0906 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 3.37218\n",
      "Epoch 500/1000\n",
      "\n",
      "Epoch 00500: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7708 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7956 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 3.37218\n",
      "Epoch 501/1000\n",
      "\n",
      "Epoch 00501: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7812 - acc: 0.9866 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.4287 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 3.37218\n",
      "Epoch 502/1000\n",
      "\n",
      "Epoch 00502: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7805 - acc: 0.9869 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8150 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 3.37218\n",
      "Epoch 503/1000\n",
      "\n",
      "Epoch 00503: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7835 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9732 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 3.37218\n",
      "Epoch 504/1000\n",
      "\n",
      "Epoch 00504: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7767 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.4365 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7406\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 3.37218\n",
      "Epoch 505/1000\n",
      "\n",
      "Epoch 00505: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7770 - acc: 0.9862 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1889 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 3.37218\n",
      "Epoch 506/1000\n",
      "\n",
      "Epoch 00506: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7759 - acc: 0.9875 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0662 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 3.37218\n",
      "Epoch 507/1000\n",
      "\n",
      "Epoch 00507: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.7801 - acc: 0.9853 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4629 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.7531\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 3.37218\n",
      "Epoch 508/1000\n",
      "\n",
      "Epoch 00508: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7711 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1346 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 3.37218\n",
      "Epoch 509/1000\n",
      "\n",
      "Epoch 00509: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7735 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8446 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 3.37218\n",
      "Epoch 510/1000\n",
      "\n",
      "Epoch 00510: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 0.7744 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0928 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 3.37218\n",
      "Epoch 511/1000\n",
      "\n",
      "Epoch 00511: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7728 - acc: 0.9872 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9245 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 3.37218\n",
      "Epoch 512/1000\n",
      "\n",
      "Epoch 00512: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7633 - acc: 0.9894 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9220 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 3.37218\n",
      "Epoch 513/1000\n",
      "\n",
      "Epoch 00513: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7748 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1486 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 3.37218\n",
      "Epoch 514/1000\n",
      "\n",
      "Epoch 00514: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7789 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9234 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 3.37218\n",
      "Epoch 515/1000\n",
      "\n",
      "Epoch 00515: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7837 - acc: 0.9862 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.3957 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 3.37218\n",
      "Epoch 516/1000\n",
      "\n",
      "Epoch 00516: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7732 - acc: 0.9862 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0538 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 3.37218\n",
      "Epoch 517/1000\n",
      "\n",
      "Epoch 00517: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7747 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6908 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 3.37218\n",
      "Epoch 518/1000\n",
      "\n",
      "Epoch 00518: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7738 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.3877 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 3.37218\n",
      "Epoch 519/1000\n",
      "\n",
      "Epoch 00519: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7716 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1475 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 3.37218\n",
      "Epoch 520/1000\n",
      "\n",
      "Epoch 00520: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7712 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9235 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 3.37218\n",
      "Epoch 521/1000\n",
      "\n",
      "Epoch 00521: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7754 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1779 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 3.37218\n",
      "Epoch 522/1000\n",
      "\n",
      "Epoch 00522: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7740 - acc: 0.9878 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.8108 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 3.37218\n",
      "Epoch 523/1000\n",
      "\n",
      "Epoch 00523: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7826 - acc: 0.9834 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8877 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 3.37218\n",
      "Epoch 524/1000\n",
      "\n",
      "Epoch 00524: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7652 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9941 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 3.37218\n",
      "Epoch 525/1000\n",
      "\n",
      "Epoch 00525: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7659 - acc: 0.9897 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.3607 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 3.37218\n",
      "Epoch 526/1000\n",
      "\n",
      "Epoch 00526: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7754 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.3158 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 3.37218\n",
      "Epoch 527/1000\n",
      "\n",
      "Epoch 00527: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7749 - acc: 0.9859 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0844 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 3.37218\n",
      "Epoch 528/1000\n",
      "\n",
      "Epoch 00528: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7685 - acc: 0.9878 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2908 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 3.37218\n",
      "Epoch 529/1000\n",
      "\n",
      "Epoch 00529: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7668 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9011 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 3.37218\n",
      "Epoch 530/1000\n",
      "\n",
      "Epoch 00530: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7658 - acc: 0.9881 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8351 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 3.37218\n",
      "Epoch 531/1000\n",
      "\n",
      "Epoch 00531: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7685 - acc: 0.9894 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0710 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 3.37218\n",
      "Epoch 532/1000\n",
      "\n",
      "Epoch 00532: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7656 - acc: 0.9912 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7965 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 3.37218\n",
      "Epoch 533/1000\n",
      "\n",
      "Epoch 00533: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7641 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6360 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7406\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 3.37218\n",
      "Epoch 534/1000\n",
      "\n",
      "Epoch 00534: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7760 - acc: 0.9847 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.2164 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 3.37218\n",
      "Epoch 535/1000\n",
      "\n",
      "Epoch 00535: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7780 - acc: 0.9847 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.2533 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 3.37218\n",
      "Epoch 536/1000\n",
      "\n",
      "Epoch 00536: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7754 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8628 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 3.37218\n",
      "Epoch 537/1000\n",
      "\n",
      "Epoch 00537: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7681 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6428 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 3.37218\n",
      "Epoch 538/1000\n",
      "\n",
      "Epoch 00538: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7695 - acc: 0.9884 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8533 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 3.37218\n",
      "Epoch 539/1000\n",
      "\n",
      "Epoch 00539: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7692 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.3518 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7344\n",
      "\n",
      "Epoch 00539: val_loss improved from 3.37218 to 3.35182, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 540/1000\n",
      "\n",
      "Epoch 00540: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7749 - acc: 0.9869 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0426 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 3.35182\n",
      "Epoch 541/1000\n",
      "\n",
      "Epoch 00541: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7809 - acc: 0.9844 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.3291 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7469\n",
      "\n",
      "Epoch 00541: val_loss improved from 3.35182 to 3.32907, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 542/1000\n",
      "\n",
      "Epoch 00542: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 0.7720 - acc: 0.9862 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9166 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 3.32907\n",
      "Epoch 543/1000\n",
      "\n",
      "Epoch 00543: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7723 - acc: 0.9859 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2110 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 3.32907\n",
      "Epoch 544/1000\n",
      "\n",
      "Epoch 00544: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7645 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1003 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 3.32907\n",
      "Epoch 545/1000\n",
      "\n",
      "Epoch 00545: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7748 - acc: 0.9884 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8959 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 3.32907\n",
      "Epoch 546/1000\n",
      "\n",
      "Epoch 00546: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7592 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.4197 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 3.32907\n",
      "Epoch 547/1000\n",
      "\n",
      "Epoch 00547: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7639 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9900 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 3.32907\n",
      "Epoch 548/1000\n",
      "\n",
      "Epoch 00548: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7609 - acc: 0.9884 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8823 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 3.32907\n",
      "Epoch 549/1000\n",
      "\n",
      "Epoch 00549: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7570 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2740 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 3.32907\n",
      "Epoch 550/1000\n",
      "\n",
      "Epoch 00550: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7649 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2654 - val_acc: 0.3937 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 3.32907\n",
      "Epoch 551/1000\n",
      "\n",
      "Epoch 00551: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7665 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2652 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 3.32907\n",
      "Epoch 552/1000\n",
      "\n",
      "Epoch 00552: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7644 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6168 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 3.32907\n",
      "Epoch 553/1000\n",
      "\n",
      "Epoch 00553: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7646 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8264 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 3.32907\n",
      "Epoch 554/1000\n",
      "\n",
      "Epoch 00554: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7645 - acc: 0.9872 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7215 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7312\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 3.32907\n",
      "Epoch 555/1000\n",
      "\n",
      "Epoch 00555: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7663 - acc: 0.9900 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.8250 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 3.32907\n",
      "Epoch 556/1000\n",
      "\n",
      "Epoch 00556: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7613 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8201 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 3.32907\n",
      "Epoch 557/1000\n",
      "\n",
      "Epoch 00557: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7621 - acc: 0.9872 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.1020 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 3.32907\n",
      "Epoch 558/1000\n",
      "\n",
      "Epoch 00558: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7645 - acc: 0.9859 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.3730 - val_acc: 0.5531 - val_top_k_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 3.32907\n",
      "Epoch 559/1000\n",
      "\n",
      "Epoch 00559: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7638 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0440 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 3.32907\n",
      "Epoch 560/1000\n",
      "\n",
      "Epoch 00560: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7624 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9707 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 3.32907\n",
      "Epoch 561/1000\n",
      "\n",
      "Epoch 00561: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7628 - acc: 0.9897 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9792 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 3.32907\n",
      "Epoch 562/1000\n",
      "\n",
      "Epoch 00562: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7648 - acc: 0.9866 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.2239 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 3.32907\n",
      "Epoch 563/1000\n",
      "\n",
      "Epoch 00563: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7606 - acc: 0.9897 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9749 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 3.32907\n",
      "Epoch 564/1000\n",
      "\n",
      "Epoch 00564: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7588 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7517 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 3.32907\n",
      "Epoch 565/1000\n",
      "\n",
      "Epoch 00565: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7652 - acc: 0.9906 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.1864 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7688\n",
      "\n",
      "Epoch 00565: val_loss improved from 3.32907 to 3.18635, saving model to saved_models/caffenet_finetuning/caffenet_single_rgb.hdf5\n",
      "Epoch 566/1000\n",
      "\n",
      "Epoch 00566: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.7637 - acc: 0.9872 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9975 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 3.18635\n",
      "Epoch 567/1000\n",
      "\n",
      "Epoch 00567: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7686 - acc: 0.9859 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.3248 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 3.18635\n",
      "Epoch 568/1000\n",
      "\n",
      "Epoch 00568: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7496 - acc: 0.9916 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8675 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 3.18635\n",
      "Epoch 569/1000\n",
      "\n",
      "Epoch 00569: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7640 - acc: 0.9872 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.6878 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 3.18635\n",
      "Epoch 570/1000\n",
      "\n",
      "Epoch 00570: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7562 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8261 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 3.18635\n",
      "Epoch 571/1000\n",
      "\n",
      "Epoch 00571: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7663 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8951 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 3.18635\n",
      "Epoch 572/1000\n",
      "\n",
      "Epoch 00572: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7760 - acc: 0.9825 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.3894 - val_acc: 0.5312 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 3.18635\n",
      "Epoch 573/1000\n",
      "\n",
      "Epoch 00573: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7560 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6678 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 3.18635\n",
      "Epoch 574/1000\n",
      "\n",
      "Epoch 00574: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7557 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1879 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 3.18635\n",
      "Epoch 575/1000\n",
      "\n",
      "Epoch 00575: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7655 - acc: 0.9866 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8228 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 3.18635\n",
      "Epoch 576/1000\n",
      "\n",
      "Epoch 00576: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7595 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0501 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 3.18635\n",
      "Epoch 577/1000\n",
      "\n",
      "Epoch 00577: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7623 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8735 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 3.18635\n",
      "Epoch 578/1000\n",
      "\n",
      "Epoch 00578: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7570 - acc: 0.9862 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7396 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 3.18635\n",
      "Epoch 579/1000\n",
      "\n",
      "Epoch 00579: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7668 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.6428 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 3.18635\n",
      "Epoch 580/1000\n",
      "\n",
      "Epoch 00580: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7603 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9124 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 3.18635\n",
      "Epoch 581/1000\n",
      "\n",
      "Epoch 00581: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7640 - acc: 0.9866 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8640 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 3.18635\n",
      "Epoch 582/1000\n",
      "\n",
      "Epoch 00582: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7548 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6293 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 3.18635\n",
      "Epoch 583/1000\n",
      "\n",
      "Epoch 00583: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7580 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.3911 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 3.18635\n",
      "Epoch 584/1000\n",
      "\n",
      "Epoch 00584: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7660 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7718 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 3.18635\n",
      "Epoch 585/1000\n",
      "\n",
      "Epoch 00585: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7585 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.4335 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 3.18635\n",
      "Epoch 586/1000\n",
      "\n",
      "Epoch 00586: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7559 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1573 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 3.18635\n",
      "Epoch 587/1000\n",
      "\n",
      "Epoch 00587: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7577 - acc: 0.9884 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7535 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 3.18635\n",
      "Epoch 588/1000\n",
      "\n",
      "Epoch 00588: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7642 - acc: 0.9872 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7497 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 3.18635\n",
      "Epoch 589/1000\n",
      "\n",
      "Epoch 00589: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7537 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6758 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 3.18635\n",
      "Epoch 590/1000\n",
      "\n",
      "Epoch 00590: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7596 - acc: 0.9875 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1766 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6312\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 3.18635\n",
      "Epoch 591/1000\n",
      "\n",
      "Epoch 00591: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7543 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.4622 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 3.18635\n",
      "Epoch 592/1000\n",
      "\n",
      "Epoch 00592: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7515 - acc: 0.9912 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1016 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 3.18635\n",
      "Epoch 593/1000\n",
      "\n",
      "Epoch 00593: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 36s 364ms/step - loss: 0.7566 - acc: 0.9884 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1038 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 3.18635\n",
      "Epoch 594/1000\n",
      "\n",
      "Epoch 00594: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7450 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0752 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 3.18635\n",
      "Epoch 595/1000\n",
      "\n",
      "Epoch 00595: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7554 - acc: 0.9872 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1300 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 3.18635\n",
      "Epoch 596/1000\n",
      "\n",
      "Epoch 00596: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7509 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9140 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 3.18635\n",
      "Epoch 597/1000\n",
      "\n",
      "Epoch 00597: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 0.7601 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.5067 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 3.18635\n",
      "Epoch 598/1000\n",
      "\n",
      "Epoch 00598: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7525 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1695 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 3.18635\n",
      "Epoch 599/1000\n",
      "\n",
      "Epoch 00599: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7449 - acc: 0.9931 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7974 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 3.18635\n",
      "Epoch 600/1000\n",
      "\n",
      "Epoch 00600: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7539 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8699 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 3.18635\n",
      "Epoch 601/1000\n",
      "\n",
      "Epoch 00601: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7571 - acc: 0.9884 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9383 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 3.18635\n",
      "Epoch 602/1000\n",
      "\n",
      "Epoch 00602: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7535 - acc: 0.9912 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.8163 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.6094\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 3.18635\n",
      "Epoch 603/1000\n",
      "\n",
      "Epoch 00603: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7571 - acc: 0.9875 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7210 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 3.18635\n",
      "Epoch 604/1000\n",
      "\n",
      "Epoch 00604: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7570 - acc: 0.9881 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6716 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 3.18635\n",
      "Epoch 605/1000\n",
      "\n",
      "Epoch 00605: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7520 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8846 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 3.18635\n",
      "Epoch 606/1000\n",
      "\n",
      "Epoch 00606: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7461 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0248 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 3.18635\n",
      "Epoch 607/1000\n",
      "\n",
      "Epoch 00607: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7516 - acc: 0.9894 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7902 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 3.18635\n",
      "Epoch 608/1000\n",
      "\n",
      "Epoch 00608: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7587 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7270 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 3.18635\n",
      "Epoch 609/1000\n",
      "\n",
      "Epoch 00609: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7504 - acc: 0.9925 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.5849 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 3.18635\n",
      "Epoch 610/1000\n",
      "\n",
      "Epoch 00610: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7489 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.4480 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 3.18635\n",
      "Epoch 611/1000\n",
      "\n",
      "Epoch 00611: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7458 - acc: 0.9909 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8533 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 3.18635\n",
      "Epoch 612/1000\n",
      "\n",
      "Epoch 00612: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7539 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8964 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 3.18635\n",
      "Epoch 613/1000\n",
      "\n",
      "Epoch 00613: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7477 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0454 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 3.18635\n",
      "Epoch 614/1000\n",
      "\n",
      "Epoch 00614: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7550 - acc: 0.9853 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5583 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 3.18635\n",
      "Epoch 615/1000\n",
      "\n",
      "Epoch 00615: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7510 - acc: 0.9906 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.3704 - val_acc: 0.4062 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 3.18635\n",
      "Epoch 616/1000\n",
      "\n",
      "Epoch 00616: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7541 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8428 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 3.18635\n",
      "Epoch 617/1000\n",
      "\n",
      "Epoch 00617: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7485 - acc: 0.9884 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1528 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 3.18635\n",
      "Epoch 618/1000\n",
      "\n",
      "Epoch 00618: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7549 - acc: 0.9878 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9183 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 3.18635\n",
      "Epoch 619/1000\n",
      "\n",
      "Epoch 00619: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7448 - acc: 0.9903 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9230 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 3.18635\n",
      "Epoch 620/1000\n",
      "\n",
      "Epoch 00620: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7531 - acc: 0.9859 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1463 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 3.18635\n",
      "Epoch 621/1000\n",
      "\n",
      "Epoch 00621: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7409 - acc: 0.9944 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8242 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 3.18635\n",
      "Epoch 622/1000\n",
      "\n",
      "Epoch 00622: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7435 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.5235 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 3.18635\n",
      "Epoch 623/1000\n",
      "\n",
      "Epoch 00623: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7497 - acc: 0.9884 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8436 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 3.18635\n",
      "Epoch 624/1000\n",
      "\n",
      "Epoch 00624: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7441 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8830 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 3.18635\n",
      "Epoch 625/1000\n",
      "\n",
      "Epoch 00625: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7484 - acc: 0.9903 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8799 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 3.18635\n",
      "Epoch 626/1000\n",
      "\n",
      "Epoch 00626: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7515 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1336 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 3.18635\n",
      "Epoch 627/1000\n",
      "\n",
      "Epoch 00627: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7507 - acc: 0.9878 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9262 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 3.18635\n",
      "Epoch 628/1000\n",
      "\n",
      "Epoch 00628: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7568 - acc: 0.9847 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8133 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 3.18635\n",
      "Epoch 629/1000\n",
      "\n",
      "Epoch 00629: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7564 - acc: 0.9875 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7860 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 3.18635\n",
      "Epoch 630/1000\n",
      "\n",
      "Epoch 00630: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7482 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0257 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 3.18635\n",
      "Epoch 631/1000\n",
      "\n",
      "Epoch 00631: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7445 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7267 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 3.18635\n",
      "Epoch 632/1000\n",
      "\n",
      "Epoch 00632: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7564 - acc: 0.9869 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0096 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 3.18635\n",
      "Epoch 633/1000\n",
      "\n",
      "Epoch 00633: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7563 - acc: 0.9878 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4655 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 3.18635\n",
      "Epoch 634/1000\n",
      "\n",
      "Epoch 00634: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7517 - acc: 0.9884 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6613 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 3.18635\n",
      "Epoch 635/1000\n",
      "\n",
      "Epoch 00635: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7541 - acc: 0.9878 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9350 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 3.18635\n",
      "Epoch 636/1000\n",
      "\n",
      "Epoch 00636: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7497 - acc: 0.9881 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8822 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 3.18635\n",
      "Epoch 637/1000\n",
      "\n",
      "Epoch 00637: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7467 - acc: 0.9884 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8688 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 3.18635\n",
      "Epoch 638/1000\n",
      "\n",
      "Epoch 00638: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7548 - acc: 0.9875 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7987 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 3.18635\n",
      "Epoch 639/1000\n",
      "\n",
      "Epoch 00639: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7512 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7063 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 3.18635\n",
      "Epoch 640/1000\n",
      "\n",
      "Epoch 00640: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7382 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5085 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7344\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 3.18635\n",
      "Epoch 641/1000\n",
      "\n",
      "Epoch 00641: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7379 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.1772 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 3.18635\n",
      "Epoch 642/1000\n",
      "\n",
      "Epoch 00642: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7458 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8339 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 3.18635\n",
      "Epoch 643/1000\n",
      "\n",
      "Epoch 00643: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7506 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8619 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 3.18635\n",
      "Epoch 644/1000\n",
      "\n",
      "Epoch 00644: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7423 - acc: 0.9922 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8136 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 3.18635\n",
      "Epoch 645/1000\n",
      "\n",
      "Epoch 00645: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7441 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6260 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 3.18635\n",
      "Epoch 646/1000\n",
      "\n",
      "Epoch 00646: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7421 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9515 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 3.18635\n",
      "Epoch 647/1000\n",
      "\n",
      "Epoch 00647: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7342 - acc: 0.9938 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6235 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 3.18635\n",
      "Epoch 648/1000\n",
      "\n",
      "Epoch 00648: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7444 - acc: 0.9894 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9765 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 3.18635\n",
      "Epoch 649/1000\n",
      "\n",
      "Epoch 00649: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7407 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.3780 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6219\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 3.18635\n",
      "Epoch 650/1000\n",
      "\n",
      "Epoch 00650: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7414 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7371 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 3.18635\n",
      "Epoch 651/1000\n",
      "\n",
      "Epoch 00651: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7378 - acc: 0.9919 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0048 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 3.18635\n",
      "Epoch 652/1000\n",
      "\n",
      "Epoch 00652: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7444 - acc: 0.9875 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7743 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 3.18635\n",
      "Epoch 653/1000\n",
      "\n",
      "Epoch 00653: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7387 - acc: 0.9931 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7804 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 3.18635\n",
      "Epoch 654/1000\n",
      "\n",
      "Epoch 00654: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7429 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0467 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 3.18635\n",
      "Epoch 655/1000\n",
      "\n",
      "Epoch 00655: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7460 - acc: 0.9891 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8863 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 3.18635\n",
      "Epoch 656/1000\n",
      "\n",
      "Epoch 00656: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7445 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9696 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 3.18635\n",
      "Epoch 657/1000\n",
      "\n",
      "Epoch 00657: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7428 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7097 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 3.18635\n",
      "Epoch 658/1000\n",
      "\n",
      "Epoch 00658: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.7453 - acc: 0.9862 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7596 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 3.18635\n",
      "Epoch 659/1000\n",
      "\n",
      "Epoch 00659: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7407 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7392 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 3.18635\n",
      "Epoch 660/1000\n",
      "\n",
      "Epoch 00660: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7439 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9970 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 3.18635\n",
      "Epoch 661/1000\n",
      "\n",
      "Epoch 00661: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7472 - acc: 0.9897 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9130 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 3.18635\n",
      "Epoch 662/1000\n",
      "\n",
      "Epoch 00662: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7484 - acc: 0.9838 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0914 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 3.18635\n",
      "Epoch 663/1000\n",
      "\n",
      "Epoch 00663: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7412 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0944 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 3.18635\n",
      "Epoch 664/1000\n",
      "\n",
      "Epoch 00664: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7398 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2509 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 3.18635\n",
      "Epoch 665/1000\n",
      "\n",
      "Epoch 00665: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7433 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7480 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 3.18635\n",
      "Epoch 666/1000\n",
      "\n",
      "Epoch 00666: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7431 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0311 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 3.18635\n",
      "Epoch 667/1000\n",
      "\n",
      "Epoch 00667: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7447 - acc: 0.9875 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6080 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7406\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 3.18635\n",
      "Epoch 668/1000\n",
      "\n",
      "Epoch 00668: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7432 - acc: 0.9909 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.9427 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 3.18635\n",
      "Epoch 669/1000\n",
      "\n",
      "Epoch 00669: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7390 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0388 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 3.18635\n",
      "Epoch 670/1000\n",
      "\n",
      "Epoch 00670: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7381 - acc: 0.9884 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1559 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 3.18635\n",
      "Epoch 671/1000\n",
      "\n",
      "Epoch 00671: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7432 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8719 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 3.18635\n",
      "Epoch 672/1000\n",
      "\n",
      "Epoch 00672: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7436 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9317 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 3.18635\n",
      "Epoch 673/1000\n",
      "\n",
      "Epoch 00673: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7454 - acc: 0.9875 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0603 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 3.18635\n",
      "Epoch 674/1000\n",
      "\n",
      "Epoch 00674: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7410 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1222 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 3.18635\n",
      "Epoch 675/1000\n",
      "\n",
      "Epoch 00675: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7415 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1061 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 3.18635\n",
      "Epoch 676/1000\n",
      "\n",
      "Epoch 00676: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7430 - acc: 0.9878 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1664 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 3.18635\n",
      "Epoch 677/1000\n",
      "\n",
      "Epoch 00677: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7354 - acc: 0.9919 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8830 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 3.18635\n",
      "Epoch 678/1000\n",
      "\n",
      "Epoch 00678: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7420 - acc: 0.9909 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8662 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 3.18635\n",
      "Epoch 679/1000\n",
      "\n",
      "Epoch 00679: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7411 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.3201 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 3.18635\n",
      "Epoch 680/1000\n",
      "\n",
      "Epoch 00680: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7451 - acc: 0.9891 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.9644 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 3.18635\n",
      "Epoch 681/1000\n",
      "\n",
      "Epoch 00681: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7439 - acc: 0.9875 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0396 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 3.18635\n",
      "Epoch 682/1000\n",
      "\n",
      "Epoch 00682: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7485 - acc: 0.9875 - top_k_categorical_accuracy: 0.9978 - val_loss: 3.6509 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 3.18635\n",
      "Epoch 683/1000\n",
      "\n",
      "Epoch 00683: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7354 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9170 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 3.18635\n",
      "Epoch 684/1000\n",
      "\n",
      "Epoch 00684: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7413 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8697 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 3.18635\n",
      "Epoch 685/1000\n",
      "\n",
      "Epoch 00685: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7346 - acc: 0.9919 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8885 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 3.18635\n",
      "Epoch 686/1000\n",
      "\n",
      "Epoch 00686: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7332 - acc: 0.9928 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8275 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 3.18635\n",
      "Epoch 687/1000\n",
      "\n",
      "Epoch 00687: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7353 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2043 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 3.18635\n",
      "Epoch 688/1000\n",
      "\n",
      "Epoch 00688: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7447 - acc: 0.9878 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9521 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 3.18635\n",
      "Epoch 689/1000\n",
      "\n",
      "Epoch 00689: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7351 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0353 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 3.18635\n",
      "Epoch 690/1000\n",
      "\n",
      "Epoch 00690: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7403 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7175 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 3.18635\n",
      "Epoch 691/1000\n",
      "\n",
      "Epoch 00691: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7339 - acc: 0.9887 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0397 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 3.18635\n",
      "Epoch 692/1000\n",
      "\n",
      "Epoch 00692: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7343 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9236 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 3.18635\n",
      "Epoch 693/1000\n",
      "\n",
      "Epoch 00693: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7333 - acc: 0.9903 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6498 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 3.18635\n",
      "Epoch 694/1000\n",
      "\n",
      "Epoch 00694: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7365 - acc: 0.9903 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4757 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 3.18635\n",
      "Epoch 695/1000\n",
      "\n",
      "Epoch 00695: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7420 - acc: 0.9884 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7797 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 3.18635\n",
      "Epoch 696/1000\n",
      "\n",
      "Epoch 00696: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7325 - acc: 0.9922 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1797 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 3.18635\n",
      "Epoch 697/1000\n",
      "\n",
      "Epoch 00697: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7300 - acc: 0.9912 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0063 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 3.18635\n",
      "Epoch 698/1000\n",
      "\n",
      "Epoch 00698: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7378 - acc: 0.9891 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0815 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 3.18635\n",
      "Epoch 699/1000\n",
      "\n",
      "Epoch 00699: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7376 - acc: 0.9891 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6802 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 3.18635\n",
      "Epoch 700/1000\n",
      "\n",
      "Epoch 00700: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7383 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.0309 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 3.18635\n",
      "Epoch 701/1000\n",
      "\n",
      "Epoch 00701: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7415 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7055 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 3.18635\n",
      "Epoch 702/1000\n",
      "\n",
      "Epoch 00702: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7439 - acc: 0.9844 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.5298 - val_acc: 0.5250 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 3.18635\n",
      "Epoch 703/1000\n",
      "\n",
      "Epoch 00703: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7354 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.2239 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 3.18635\n",
      "Epoch 704/1000\n",
      "\n",
      "Epoch 00704: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7367 - acc: 0.9909 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.8675 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 3.18635\n",
      "Epoch 705/1000\n",
      "\n",
      "Epoch 00705: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7335 - acc: 0.9887 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0758 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 3.18635\n",
      "Epoch 706/1000\n",
      "\n",
      "Epoch 00706: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7386 - acc: 0.9875 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7179 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 3.18635\n",
      "Epoch 707/1000\n",
      "\n",
      "Epoch 00707: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7320 - acc: 0.9934 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.5549 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 3.18635\n",
      "Epoch 708/1000\n",
      "\n",
      "Epoch 00708: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7293 - acc: 0.9916 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8052 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 3.18635\n",
      "Epoch 709/1000\n",
      "\n",
      "Epoch 00709: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7325 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9846 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 3.18635\n",
      "Epoch 710/1000\n",
      "\n",
      "Epoch 00710: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7332 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0907 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 3.18635\n",
      "Epoch 711/1000\n",
      "\n",
      "Epoch 00711: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7337 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1081 - val_acc: 0.4031 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 3.18635\n",
      "Epoch 712/1000\n",
      "\n",
      "Epoch 00712: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7437 - acc: 0.9869 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7886 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 3.18635\n",
      "Epoch 713/1000\n",
      "\n",
      "Epoch 00713: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7284 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.1946 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 3.18635\n",
      "Epoch 714/1000\n",
      "\n",
      "Epoch 00714: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7465 - acc: 0.9859 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.5678 - val_acc: 0.5375 - val_top_k_categorical_accuracy: 0.7531\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 3.18635\n",
      "Epoch 715/1000\n",
      "\n",
      "Epoch 00715: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7232 - acc: 0.9938 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6543 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 3.18635\n",
      "Epoch 716/1000\n",
      "\n",
      "Epoch 00716: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7347 - acc: 0.9894 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.5880 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 3.18635\n",
      "Epoch 717/1000\n",
      "\n",
      "Epoch 00717: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7397 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8181 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 3.18635\n",
      "Epoch 718/1000\n",
      "\n",
      "Epoch 00718: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7353 - acc: 0.9878 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0942 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 3.18635\n",
      "Epoch 719/1000\n",
      "\n",
      "Epoch 00719: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7438 - acc: 0.9862 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0844 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 3.18635\n",
      "Epoch 720/1000\n",
      "\n",
      "Epoch 00720: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7339 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9436 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 3.18635\n",
      "Epoch 721/1000\n",
      "\n",
      "Epoch 00721: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7389 - acc: 0.9859 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9401 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 3.18635\n",
      "Epoch 722/1000\n",
      "\n",
      "Epoch 00722: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7349 - acc: 0.9872 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7453 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 3.18635\n",
      "Epoch 723/1000\n",
      "\n",
      "Epoch 00723: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7353 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0628 - val_acc: 0.4219 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 3.18635\n",
      "Epoch 724/1000\n",
      "\n",
      "Epoch 00724: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.7403 - acc: 0.9881 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0353 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 3.18635\n",
      "Epoch 725/1000\n",
      "\n",
      "Epoch 00725: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7407 - acc: 0.9862 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.9784 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 3.18635\n",
      "Epoch 726/1000\n",
      "\n",
      "Epoch 00726: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7406 - acc: 0.9859 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9294 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 3.18635\n",
      "Epoch 727/1000\n",
      "\n",
      "Epoch 00727: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7324 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9633 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 3.18635\n",
      "Epoch 728/1000\n",
      "\n",
      "Epoch 00728: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7304 - acc: 0.9922 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7709 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 3.18635\n",
      "Epoch 729/1000\n",
      "\n",
      "Epoch 00729: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7373 - acc: 0.9869 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.2846 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6375\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 3.18635\n",
      "Epoch 730/1000\n",
      "\n",
      "Epoch 00730: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7342 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8327 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 3.18635\n",
      "Epoch 731/1000\n",
      "\n",
      "Epoch 00731: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7257 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8418 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 3.18635\n",
      "Epoch 732/1000\n",
      "\n",
      "Epoch 00732: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7329 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.4699 - val_acc: 0.4188 - val_top_k_categorical_accuracy: 0.6125\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 3.18635\n",
      "Epoch 733/1000\n",
      "\n",
      "Epoch 00733: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7290 - acc: 0.9894 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9118 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 3.18635\n",
      "Epoch 734/1000\n",
      "\n",
      "Epoch 00734: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7224 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7050 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 3.18635\n",
      "Epoch 735/1000\n",
      "\n",
      "Epoch 00735: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7352 - acc: 0.9859 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1599 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 3.18635\n",
      "Epoch 736/1000\n",
      "\n",
      "Epoch 00736: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7251 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1436 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 3.18635\n",
      "Epoch 737/1000\n",
      "\n",
      "Epoch 00737: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7243 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7483 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 3.18635\n",
      "Epoch 738/1000\n",
      "\n",
      "Epoch 00738: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7203 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7639 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 3.18635\n",
      "Epoch 739/1000\n",
      "\n",
      "Epoch 00739: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7392 - acc: 0.9847 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.5766 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7438\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 3.18635\n",
      "Epoch 740/1000\n",
      "\n",
      "Epoch 00740: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7289 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8632 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 3.18635\n",
      "Epoch 741/1000\n",
      "\n",
      "Epoch 00741: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7260 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9125 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 3.18635\n",
      "Epoch 742/1000\n",
      "\n",
      "Epoch 00742: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7237 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0823 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 3.18635\n",
      "Epoch 743/1000\n",
      "\n",
      "Epoch 00743: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7361 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0993 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 3.18635\n",
      "Epoch 744/1000\n",
      "\n",
      "Epoch 00744: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7271 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6343 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 3.18635\n",
      "Epoch 745/1000\n",
      "\n",
      "Epoch 00745: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7295 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9779 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 3.18635\n",
      "Epoch 746/1000\n",
      "\n",
      "Epoch 00746: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7266 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9886 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 3.18635\n",
      "Epoch 747/1000\n",
      "\n",
      "Epoch 00747: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7335 - acc: 0.9872 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8854 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 3.18635\n",
      "Epoch 748/1000\n",
      "\n",
      "Epoch 00748: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7269 - acc: 0.9875 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0333 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 3.18635\n",
      "Epoch 749/1000\n",
      "\n",
      "Epoch 00749: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7225 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7375 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 3.18635\n",
      "Epoch 750/1000\n",
      "\n",
      "Epoch 00750: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7330 - acc: 0.9862 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8655 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 3.18635\n",
      "Epoch 751/1000\n",
      "\n",
      "Epoch 00751: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7286 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6412 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 3.18635\n",
      "Epoch 752/1000\n",
      "\n",
      "Epoch 00752: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7294 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6797 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 3.18635\n",
      "Epoch 753/1000\n",
      "\n",
      "Epoch 00753: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7312 - acc: 0.9887 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.1403 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 3.18635\n",
      "Epoch 754/1000\n",
      "\n",
      "Epoch 00754: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7224 - acc: 0.9916 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8733 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 3.18635\n",
      "Epoch 755/1000\n",
      "\n",
      "Epoch 00755: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7213 - acc: 0.9925 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6290 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 3.18635\n",
      "Epoch 756/1000\n",
      "\n",
      "Epoch 00756: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7234 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0240 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 3.18635\n",
      "Epoch 757/1000\n",
      "\n",
      "Epoch 00757: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7214 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7808 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 3.18635\n",
      "Epoch 758/1000\n",
      "\n",
      "Epoch 00758: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7251 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7776 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 3.18635\n",
      "Epoch 759/1000\n",
      "\n",
      "Epoch 00759: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7268 - acc: 0.9872 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2063 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 3.18635\n",
      "Epoch 760/1000\n",
      "\n",
      "Epoch 00760: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7284 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9772 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 3.18635\n",
      "Epoch 761/1000\n",
      "\n",
      "Epoch 00761: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7244 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1929 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 3.18635\n",
      "Epoch 762/1000\n",
      "\n",
      "Epoch 00762: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7287 - acc: 0.9900 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0921 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 3.18635\n",
      "Epoch 763/1000\n",
      "\n",
      "Epoch 00763: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7281 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8845 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 3.18635\n",
      "Epoch 764/1000\n",
      "\n",
      "Epoch 00764: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7277 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7824 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 3.18635\n",
      "Epoch 765/1000\n",
      "\n",
      "Epoch 00765: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7356 - acc: 0.9859 - top_k_categorical_accuracy: 0.9978 - val_loss: 4.2265 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 3.18635\n",
      "Epoch 766/1000\n",
      "\n",
      "Epoch 00766: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7275 - acc: 0.9875 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6595 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 3.18635\n",
      "Epoch 767/1000\n",
      "\n",
      "Epoch 00767: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7203 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7155 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 3.18635\n",
      "Epoch 768/1000\n",
      "\n",
      "Epoch 00768: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7215 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6495 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 3.18635\n",
      "Epoch 769/1000\n",
      "\n",
      "Epoch 00769: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7236 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7259 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 3.18635\n",
      "Epoch 770/1000\n",
      "\n",
      "Epoch 00770: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7199 - acc: 0.9916 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1252 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 3.18635\n",
      "Epoch 771/1000\n",
      "\n",
      "Epoch 00771: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7216 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7032 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 3.18635\n",
      "Epoch 772/1000\n",
      "\n",
      "Epoch 00772: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7268 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9975 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 3.18635\n",
      "Epoch 773/1000\n",
      "\n",
      "Epoch 00773: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7161 - acc: 0.9934 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.4862 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 3.18635\n",
      "Epoch 774/1000\n",
      "\n",
      "Epoch 00774: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7188 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1825 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 3.18635\n",
      "Epoch 775/1000\n",
      "\n",
      "Epoch 00775: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7184 - acc: 0.9916 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1691 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 3.18635\n",
      "Epoch 776/1000\n",
      "\n",
      "Epoch 00776: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7181 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5524 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 3.18635\n",
      "Epoch 777/1000\n",
      "\n",
      "Epoch 00777: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7340 - acc: 0.9866 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7306 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 3.18635\n",
      "Epoch 778/1000\n",
      "\n",
      "Epoch 00778: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7174 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7901 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 3.18635\n",
      "Epoch 779/1000\n",
      "\n",
      "Epoch 00779: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7261 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.3528 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 3.18635\n",
      "Epoch 780/1000\n",
      "\n",
      "Epoch 00780: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7193 - acc: 0.9922 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0115 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 3.18635\n",
      "Epoch 781/1000\n",
      "\n",
      "Epoch 00781: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7243 - acc: 0.9906 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0573 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 3.18635\n",
      "Epoch 782/1000\n",
      "\n",
      "Epoch 00782: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7197 - acc: 0.9897 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7204 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7312\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 3.18635\n",
      "Epoch 783/1000\n",
      "\n",
      "Epoch 00783: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7135 - acc: 0.9925 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9012 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 3.18635\n",
      "Epoch 784/1000\n",
      "\n",
      "Epoch 00784: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7231 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6467 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 3.18635\n",
      "Epoch 785/1000\n",
      "\n",
      "Epoch 00785: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7306 - acc: 0.9881 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.8252 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 3.18635\n",
      "Epoch 786/1000\n",
      "\n",
      "Epoch 00786: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7217 - acc: 0.9894 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7035 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 3.18635\n",
      "Epoch 787/1000\n",
      "\n",
      "Epoch 00787: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7266 - acc: 0.9881 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5812 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 3.18635\n",
      "Epoch 788/1000\n",
      "\n",
      "Epoch 00788: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7340 - acc: 0.9853 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6404 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 3.18635\n",
      "Epoch 789/1000\n",
      "\n",
      "Epoch 00789: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.7310 - acc: 0.9850 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.8578 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 3.18635\n",
      "Epoch 790/1000\n",
      "\n",
      "Epoch 00790: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7244 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8591 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 3.18635\n",
      "Epoch 791/1000\n",
      "\n",
      "Epoch 00791: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7234 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8903 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 3.18635\n",
      "Epoch 792/1000\n",
      "\n",
      "Epoch 00792: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7236 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9861 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 3.18635\n",
      "Epoch 793/1000\n",
      "\n",
      "Epoch 00793: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7160 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0369 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 3.18635\n",
      "Epoch 794/1000\n",
      "\n",
      "Epoch 00794: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7207 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5014 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 3.18635\n",
      "Epoch 795/1000\n",
      "\n",
      "Epoch 00795: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7135 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8671 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 3.18635\n",
      "Epoch 796/1000\n",
      "\n",
      "Epoch 00796: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7173 - acc: 0.9897 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0860 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 3.18635\n",
      "Epoch 797/1000\n",
      "\n",
      "Epoch 00797: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7161 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9975 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 3.18635\n",
      "Epoch 798/1000\n",
      "\n",
      "Epoch 00798: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7100 - acc: 0.9928 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7793 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 3.18635\n",
      "Epoch 799/1000\n",
      "\n",
      "Epoch 00799: LearningRateScheduler setting learning rate to 0.0001.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7153 - acc: 0.9916 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7582 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 3.18635\n",
      "Epoch 800/1000\n",
      "\n",
      "Epoch 00800: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7142 - acc: 0.9912 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1859 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 3.18635\n",
      "Epoch 801/1000\n",
      "\n",
      "Epoch 00801: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7217 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7739 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 3.18635\n",
      "Epoch 802/1000\n",
      "\n",
      "Epoch 00802: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7148 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9936 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 3.18635\n",
      "Epoch 803/1000\n",
      "\n",
      "Epoch 00803: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7231 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8412 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 3.18635\n",
      "Epoch 804/1000\n",
      "\n",
      "Epoch 00804: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7236 - acc: 0.9903 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.3696 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 3.18635\n",
      "Epoch 805/1000\n",
      "\n",
      "Epoch 00805: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7214 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.4368 - val_acc: 0.5250 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 3.18635\n",
      "Epoch 806/1000\n",
      "\n",
      "Epoch 00806: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7162 - acc: 0.9916 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8074 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 3.18635\n",
      "Epoch 807/1000\n",
      "\n",
      "Epoch 00807: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7127 - acc: 0.9928 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7897 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 3.18635\n",
      "Epoch 808/1000\n",
      "\n",
      "Epoch 00808: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7214 - acc: 0.9887 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8276 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 3.18635\n",
      "Epoch 809/1000\n",
      "\n",
      "Epoch 00809: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7195 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9196 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 3.18635\n",
      "Epoch 810/1000\n",
      "\n",
      "Epoch 00810: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7134 - acc: 0.9912 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0240 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 3.18635\n",
      "Epoch 811/1000\n",
      "\n",
      "Epoch 00811: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7131 - acc: 0.9928 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.5297 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 3.18635\n",
      "Epoch 812/1000\n",
      "\n",
      "Epoch 00812: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7170 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8031 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 3.18635\n",
      "Epoch 813/1000\n",
      "\n",
      "Epoch 00813: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7157 - acc: 0.9897 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9771 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 3.18635\n",
      "Epoch 814/1000\n",
      "\n",
      "Epoch 00814: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7173 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5863 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00814: val_loss did not improve from 3.18635\n",
      "Epoch 815/1000\n",
      "\n",
      "Epoch 00815: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7153 - acc: 0.9909 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.4535 - val_acc: 0.5469 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 3.18635\n",
      "Epoch 816/1000\n",
      "\n",
      "Epoch 00816: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7107 - acc: 0.9922 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0321 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 3.18635\n",
      "Epoch 817/1000\n",
      "\n",
      "Epoch 00817: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7154 - acc: 0.9922 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0615 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 3.18635\n",
      "Epoch 818/1000\n",
      "\n",
      "Epoch 00818: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7166 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0422 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 3.18635\n",
      "Epoch 819/1000\n",
      "\n",
      "Epoch 00819: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7091 - acc: 0.9938 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5121 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 3.18635\n",
      "Epoch 820/1000\n",
      "\n",
      "Epoch 00820: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7227 - acc: 0.9869 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0143 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 3.18635\n",
      "Epoch 821/1000\n",
      "\n",
      "Epoch 00821: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7133 - acc: 0.9931 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7876 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 3.18635\n",
      "Epoch 822/1000\n",
      "\n",
      "Epoch 00822: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7142 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6644 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 3.18635\n",
      "Epoch 823/1000\n",
      "\n",
      "Epoch 00823: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7149 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8786 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 3.18635\n",
      "Epoch 824/1000\n",
      "\n",
      "Epoch 00824: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7144 - acc: 0.9903 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9301 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 3.18635\n",
      "Epoch 825/1000\n",
      "\n",
      "Epoch 00825: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7171 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.4572 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6156\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 3.18635\n",
      "Epoch 826/1000\n",
      "\n",
      "Epoch 00826: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7188 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0304 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 3.18635\n",
      "Epoch 827/1000\n",
      "\n",
      "Epoch 00827: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7142 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1396 - val_acc: 0.4156 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 3.18635\n",
      "Epoch 828/1000\n",
      "\n",
      "Epoch 00828: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7244 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8654 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 3.18635\n",
      "Epoch 829/1000\n",
      "\n",
      "Epoch 00829: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7067 - acc: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6510 - val_acc: 0.5344 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 3.18635\n",
      "Epoch 830/1000\n",
      "\n",
      "Epoch 00830: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7185 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6374 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 3.18635\n",
      "Epoch 831/1000\n",
      "\n",
      "Epoch 00831: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7205 - acc: 0.9887 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8688 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 3.18635\n",
      "Epoch 832/1000\n",
      "\n",
      "Epoch 00832: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7151 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.2657 - val_acc: 0.5312 - val_top_k_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 3.18635\n",
      "Epoch 833/1000\n",
      "\n",
      "Epoch 00833: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7170 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.6291 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 3.18635\n",
      "Epoch 834/1000\n",
      "\n",
      "Epoch 00834: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7161 - acc: 0.9897 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.4032 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 3.18635\n",
      "Epoch 835/1000\n",
      "\n",
      "Epoch 00835: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7228 - acc: 0.9884 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0859 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 3.18635\n",
      "Epoch 836/1000\n",
      "\n",
      "Epoch 00836: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7135 - acc: 0.9919 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.3320 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.7469\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 3.18635\n",
      "Epoch 837/1000\n",
      "\n",
      "Epoch 00837: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7114 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9405 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 3.18635\n",
      "Epoch 838/1000\n",
      "\n",
      "Epoch 00838: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7106 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0931 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 3.18635\n",
      "Epoch 839/1000\n",
      "\n",
      "Epoch 00839: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7160 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.2493 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 3.18635\n",
      "Epoch 840/1000\n",
      "\n",
      "Epoch 00840: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7156 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7767 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 3.18635\n",
      "Epoch 841/1000\n",
      "\n",
      "Epoch 00841: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7147 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.4986 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 3.18635\n",
      "Epoch 842/1000\n",
      "\n",
      "Epoch 00842: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7153 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7038 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 3.18635\n",
      "Epoch 843/1000\n",
      "\n",
      "Epoch 00843: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7152 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2954 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 3.18635\n",
      "Epoch 844/1000\n",
      "\n",
      "Epoch 00844: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7130 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9567 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 3.18635\n",
      "Epoch 845/1000\n",
      "\n",
      "Epoch 00845: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7139 - acc: 0.9897 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1085 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 3.18635\n",
      "Epoch 846/1000\n",
      "\n",
      "Epoch 00846: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7117 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6568 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 3.18635\n",
      "Epoch 847/1000\n",
      "\n",
      "Epoch 00847: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7125 - acc: 0.9916 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9008 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 3.18635\n",
      "Epoch 848/1000\n",
      "\n",
      "Epoch 00848: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7139 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8771 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 3.18635\n",
      "Epoch 849/1000\n",
      "\n",
      "Epoch 00849: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7143 - acc: 0.9916 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7536 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 3.18635\n",
      "Epoch 850/1000\n",
      "\n",
      "Epoch 00850: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7237 - acc: 0.9862 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9082 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 3.18635\n",
      "Epoch 851/1000\n",
      "\n",
      "Epoch 00851: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7224 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7871 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 3.18635\n",
      "Epoch 852/1000\n",
      "\n",
      "Epoch 00852: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7219 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1356 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 3.18635\n",
      "Epoch 853/1000\n",
      "\n",
      "Epoch 00853: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7159 - acc: 0.9916 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8735 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 3.18635\n",
      "Epoch 854/1000\n",
      "\n",
      "Epoch 00854: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7064 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9586 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 3.18635\n",
      "Epoch 855/1000\n",
      "\n",
      "Epoch 00855: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.7192 - acc: 0.9897 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8920 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 3.18635\n",
      "Epoch 856/1000\n",
      "\n",
      "Epoch 00856: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7241 - acc: 0.9881 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0964 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 3.18635\n",
      "Epoch 857/1000\n",
      "\n",
      "Epoch 00857: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7178 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.3942 - val_acc: 0.5250 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 3.18635\n",
      "Epoch 858/1000\n",
      "\n",
      "Epoch 00858: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7101 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0095 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 3.18635\n",
      "Epoch 859/1000\n",
      "\n",
      "Epoch 00859: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7169 - acc: 0.9912 - top_k_categorical_accuracy: 0.9981 - val_loss: 4.1696 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 3.18635\n",
      "Epoch 860/1000\n",
      "\n",
      "Epoch 00860: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7138 - acc: 0.9909 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8586 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 3.18635\n",
      "Epoch 861/1000\n",
      "\n",
      "Epoch 00861: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7179 - acc: 0.9875 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5339 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 3.18635\n",
      "Epoch 862/1000\n",
      "\n",
      "Epoch 00862: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7173 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8556 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 3.18635\n",
      "Epoch 863/1000\n",
      "\n",
      "Epoch 00863: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7099 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.3499 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 3.18635\n",
      "Epoch 864/1000\n",
      "\n",
      "Epoch 00864: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7151 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7792 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 3.18635\n",
      "Epoch 865/1000\n",
      "\n",
      "Epoch 00865: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7147 - acc: 0.9922 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6243 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 3.18635\n",
      "Epoch 866/1000\n",
      "\n",
      "Epoch 00866: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7194 - acc: 0.9891 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.7031 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 3.18635\n",
      "Epoch 867/1000\n",
      "\n",
      "Epoch 00867: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7094 - acc: 0.9909 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.5939 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 3.18635\n",
      "Epoch 868/1000\n",
      "\n",
      "Epoch 00868: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7164 - acc: 0.9916 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.0347 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 3.18635\n",
      "Epoch 869/1000\n",
      "\n",
      "Epoch 00869: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7121 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.4811 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 3.18635\n",
      "Epoch 870/1000\n",
      "\n",
      "Epoch 00870: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7092 - acc: 0.9925 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0675 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 3.18635\n",
      "Epoch 871/1000\n",
      "\n",
      "Epoch 00871: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7168 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.2149 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 3.18635\n",
      "Epoch 872/1000\n",
      "\n",
      "Epoch 00872: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7128 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8915 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 3.18635\n",
      "Epoch 873/1000\n",
      "\n",
      "Epoch 00873: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7093 - acc: 0.9922 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6125 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 3.18635\n",
      "Epoch 874/1000\n",
      "\n",
      "Epoch 00874: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.7098 - acc: 0.9931 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.4553 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 3.18635\n",
      "Epoch 875/1000\n",
      "\n",
      "Epoch 00875: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7180 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1267 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 3.18635\n",
      "Epoch 876/1000\n",
      "\n",
      "Epoch 00876: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7117 - acc: 0.9934 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.3997 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7219\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 3.18635\n",
      "Epoch 877/1000\n",
      "\n",
      "Epoch 00877: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7130 - acc: 0.9919 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7134 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 3.18635\n",
      "Epoch 878/1000\n",
      "\n",
      "Epoch 00878: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7151 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1747 - val_acc: 0.4406 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 3.18635\n",
      "Epoch 879/1000\n",
      "\n",
      "Epoch 00879: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7195 - acc: 0.9887 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5678 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7312\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 3.18635\n",
      "Epoch 880/1000\n",
      "\n",
      "Epoch 00880: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7184 - acc: 0.9894 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7000 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 3.18635\n",
      "Epoch 881/1000\n",
      "\n",
      "Epoch 00881: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.7130 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8606 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 3.18635\n",
      "Epoch 882/1000\n",
      "\n",
      "Epoch 00882: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.7170 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8814 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 3.18635\n",
      "Epoch 883/1000\n",
      "\n",
      "Epoch 00883: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7124 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.4687 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 3.18635\n",
      "Epoch 884/1000\n",
      "\n",
      "Epoch 00884: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7187 - acc: 0.9881 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7644 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 3.18635\n",
      "Epoch 885/1000\n",
      "\n",
      "Epoch 00885: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7164 - acc: 0.9916 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9980 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 3.18635\n",
      "Epoch 886/1000\n",
      "\n",
      "Epoch 00886: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.7109 - acc: 0.9909 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0916 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 3.18635\n",
      "Epoch 887/1000\n",
      "\n",
      "Epoch 00887: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7083 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1073 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 3.18635\n",
      "Epoch 888/1000\n",
      "\n",
      "Epoch 00888: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7163 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9574 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 3.18635\n",
      "Epoch 889/1000\n",
      "\n",
      "Epoch 00889: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7170 - acc: 0.9912 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9903 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 3.18635\n",
      "Epoch 890/1000\n",
      "\n",
      "Epoch 00890: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7128 - acc: 0.9919 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6377 - val_acc: 0.5062 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 3.18635\n",
      "Epoch 891/1000\n",
      "\n",
      "Epoch 00891: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7130 - acc: 0.9903 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1106 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 3.18635\n",
      "Epoch 892/1000\n",
      "\n",
      "Epoch 00892: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7112 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1154 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 3.18635\n",
      "Epoch 893/1000\n",
      "\n",
      "Epoch 00893: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7126 - acc: 0.9900 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8039 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 3.18635\n",
      "Epoch 894/1000\n",
      "\n",
      "Epoch 00894: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7083 - acc: 0.9931 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7717 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 3.18635\n",
      "Epoch 895/1000\n",
      "\n",
      "Epoch 00895: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7155 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8460 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 3.18635\n",
      "Epoch 896/1000\n",
      "\n",
      "Epoch 00896: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7122 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9614 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 3.18635\n",
      "Epoch 897/1000\n",
      "\n",
      "Epoch 00897: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7197 - acc: 0.9903 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8985 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 3.18635\n",
      "Epoch 898/1000\n",
      "\n",
      "Epoch 00898: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7103 - acc: 0.9919 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0305 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 3.18635\n",
      "Epoch 899/1000\n",
      "\n",
      "Epoch 00899: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7181 - acc: 0.9897 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6583 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 3.18635\n",
      "Epoch 900/1000\n",
      "\n",
      "Epoch 00900: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7110 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8973 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 3.18635\n",
      "Epoch 901/1000\n",
      "\n",
      "Epoch 00901: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7248 - acc: 0.9875 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7888 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 3.18635\n",
      "Epoch 902/1000\n",
      "\n",
      "Epoch 00902: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7116 - acc: 0.9931 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7563 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 3.18635\n",
      "Epoch 903/1000\n",
      "\n",
      "Epoch 00903: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7141 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7255 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 3.18635\n",
      "Epoch 904/1000\n",
      "\n",
      "Epoch 00904: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7053 - acc: 0.9941 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.2544 - val_acc: 0.4125 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 3.18635\n",
      "Epoch 905/1000\n",
      "\n",
      "Epoch 00905: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7083 - acc: 0.9938 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8346 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 3.18635\n",
      "Epoch 906/1000\n",
      "\n",
      "Epoch 00906: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7168 - acc: 0.9891 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1190 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 3.18635\n",
      "Epoch 907/1000\n",
      "\n",
      "Epoch 00907: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7127 - acc: 0.9916 - top_k_categorical_accuracy: 0.9984 - val_loss: 3.5771 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 3.18635\n",
      "Epoch 908/1000\n",
      "\n",
      "Epoch 00908: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7098 - acc: 0.9919 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.3398 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 3.18635\n",
      "Epoch 909/1000\n",
      "\n",
      "Epoch 00909: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7103 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5494 - val_acc: 0.5188 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 3.18635\n",
      "Epoch 910/1000\n",
      "\n",
      "Epoch 00910: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7054 - acc: 0.9928 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.0102 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 3.18635\n",
      "Epoch 911/1000\n",
      "\n",
      "Epoch 00911: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7194 - acc: 0.9891 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7024 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 3.18635\n",
      "Epoch 912/1000\n",
      "\n",
      "Epoch 00912: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7165 - acc: 0.9894 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7410 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 3.18635\n",
      "Epoch 913/1000\n",
      "\n",
      "Epoch 00913: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7176 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9068 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 3.18635\n",
      "Epoch 914/1000\n",
      "\n",
      "Epoch 00914: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7192 - acc: 0.9900 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9958 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 3.18635\n",
      "Epoch 915/1000\n",
      "\n",
      "Epoch 00915: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7126 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6722 - val_acc: 0.5437 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 3.18635\n",
      "Epoch 916/1000\n",
      "\n",
      "Epoch 00916: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7114 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9780 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 3.18635\n",
      "Epoch 917/1000\n",
      "\n",
      "Epoch 00917: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7187 - acc: 0.9881 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9360 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 3.18635\n",
      "Epoch 918/1000\n",
      "\n",
      "Epoch 00918: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7129 - acc: 0.9931 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8760 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 3.18635\n",
      "Epoch 919/1000\n",
      "\n",
      "Epoch 00919: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7071 - acc: 0.9941 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5367 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 3.18635\n",
      "Epoch 920/1000\n",
      "\n",
      "Epoch 00920: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7123 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0359 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 3.18635\n",
      "Epoch 921/1000\n",
      "\n",
      "Epoch 00921: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7098 - acc: 0.9928 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9253 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 3.18635\n",
      "Epoch 922/1000\n",
      "\n",
      "Epoch 00922: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7187 - acc: 0.9866 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.4938 - val_acc: 0.4969 - val_top_k_categorical_accuracy: 0.7562\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 3.18635\n",
      "Epoch 923/1000\n",
      "\n",
      "Epoch 00923: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7110 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.1147 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 3.18635\n",
      "Epoch 924/1000\n",
      "\n",
      "Epoch 00924: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7156 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7655 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 3.18635\n",
      "Epoch 925/1000\n",
      "\n",
      "Epoch 00925: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7160 - acc: 0.9891 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.7945 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 3.18635\n",
      "Epoch 926/1000\n",
      "\n",
      "Epoch 00926: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7127 - acc: 0.9912 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.4630 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 3.18635\n",
      "Epoch 927/1000\n",
      "\n",
      "Epoch 00927: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7106 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9282 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 3.18635\n",
      "Epoch 928/1000\n",
      "\n",
      "Epoch 00928: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7116 - acc: 0.9919 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8101 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 3.18635\n",
      "Epoch 929/1000\n",
      "\n",
      "Epoch 00929: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7186 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8398 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 3.18635\n",
      "Epoch 930/1000\n",
      "\n",
      "Epoch 00930: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7069 - acc: 0.9925 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.5193 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.7531\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 3.18635\n",
      "Epoch 931/1000\n",
      "\n",
      "Epoch 00931: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7133 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.3941 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 3.18635\n",
      "Epoch 932/1000\n",
      "\n",
      "Epoch 00932: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7162 - acc: 0.9903 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6105 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 3.18635\n",
      "Epoch 933/1000\n",
      "\n",
      "Epoch 00933: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.7129 - acc: 0.9922 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0880 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 3.18635\n",
      "Epoch 934/1000\n",
      "\n",
      "Epoch 00934: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7116 - acc: 0.9919 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.3229 - val_acc: 0.3875 - val_top_k_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 3.18635\n",
      "Epoch 935/1000\n",
      "\n",
      "Epoch 00935: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7147 - acc: 0.9912 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.8179 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 3.18635\n",
      "Epoch 936/1000\n",
      "\n",
      "Epoch 00936: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7116 - acc: 0.9928 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8349 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 3.18635\n",
      "Epoch 937/1000\n",
      "\n",
      "Epoch 00937: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7096 - acc: 0.9928 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1443 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 3.18635\n",
      "Epoch 938/1000\n",
      "\n",
      "Epoch 00938: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7050 - acc: 0.9947 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7885 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 3.18635\n",
      "Epoch 939/1000\n",
      "\n",
      "Epoch 00939: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7132 - acc: 0.9931 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0153 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6562\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 3.18635\n",
      "Epoch 940/1000\n",
      "\n",
      "Epoch 00940: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7114 - acc: 0.9931 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9462 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 3.18635\n",
      "Epoch 941/1000\n",
      "\n",
      "Epoch 00941: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7145 - acc: 0.9922 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.4895 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 3.18635\n",
      "Epoch 942/1000\n",
      "\n",
      "Epoch 00942: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7143 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.6253 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7312\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 3.18635\n",
      "Epoch 943/1000\n",
      "\n",
      "Epoch 00943: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7146 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0362 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 3.18635\n",
      "Epoch 944/1000\n",
      "\n",
      "Epoch 00944: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7152 - acc: 0.9878 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6431 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 3.18635\n",
      "Epoch 945/1000\n",
      "\n",
      "Epoch 00945: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7113 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6937 - val_acc: 0.5250 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 3.18635\n",
      "Epoch 946/1000\n",
      "\n",
      "Epoch 00946: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7058 - acc: 0.9916 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8020 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 3.18635\n",
      "Epoch 947/1000\n",
      "\n",
      "Epoch 00947: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.7136 - acc: 0.9912 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9616 - val_acc: 0.4500 - val_top_k_categorical_accuracy: 0.7063\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 3.18635\n",
      "Epoch 948/1000\n",
      "\n",
      "Epoch 00948: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7179 - acc: 0.9884 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.9379 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6531\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 3.18635\n",
      "Epoch 949/1000\n",
      "\n",
      "Epoch 00949: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7117 - acc: 0.9891 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.9765 - val_acc: 0.4813 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 3.18635\n",
      "Epoch 950/1000\n",
      "\n",
      "Epoch 00950: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7135 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6647 - val_acc: 0.5219 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 3.18635\n",
      "Epoch 951/1000\n",
      "\n",
      "Epoch 00951: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7108 - acc: 0.9909 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6372 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 3.18635\n",
      "Epoch 952/1000\n",
      "\n",
      "Epoch 00952: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7159 - acc: 0.9891 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0130 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 3.18635\n",
      "Epoch 953/1000\n",
      "\n",
      "Epoch 00953: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7105 - acc: 0.9941 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7228 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 3.18635\n",
      "Epoch 954/1000\n",
      "\n",
      "Epoch 00954: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7173 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1151 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 3.18635\n",
      "Epoch 955/1000\n",
      "\n",
      "Epoch 00955: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7130 - acc: 0.9919 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.9997 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 3.18635\n",
      "Epoch 956/1000\n",
      "\n",
      "Epoch 00956: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7149 - acc: 0.9897 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.1058 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 3.18635\n",
      "Epoch 957/1000\n",
      "\n",
      "Epoch 00957: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7094 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8685 - val_acc: 0.4313 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 3.18635\n",
      "Epoch 958/1000\n",
      "\n",
      "Epoch 00958: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7130 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6976 - val_acc: 0.4844 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 3.18635\n",
      "Epoch 959/1000\n",
      "\n",
      "Epoch 00959: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7180 - acc: 0.9881 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1434 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 3.18635\n",
      "Epoch 960/1000\n",
      "\n",
      "Epoch 00960: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7109 - acc: 0.9906 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7340 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 3.18635\n",
      "Epoch 961/1000\n",
      "\n",
      "Epoch 00961: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7101 - acc: 0.9916 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0536 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 3.18635\n",
      "Epoch 962/1000\n",
      "\n",
      "Epoch 00962: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7207 - acc: 0.9884 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9566 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 3.18635\n",
      "Epoch 963/1000\n",
      "\n",
      "Epoch 00963: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7158 - acc: 0.9906 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0486 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 3.18635\n",
      "Epoch 964/1000\n",
      "\n",
      "Epoch 00964: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7109 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7168 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 3.18635\n",
      "Epoch 965/1000\n",
      "\n",
      "Epoch 00965: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.7156 - acc: 0.9900 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.1572 - val_acc: 0.4531 - val_top_k_categorical_accuracy: 0.6469\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 3.18635\n",
      "Epoch 966/1000\n",
      "\n",
      "Epoch 00966: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7124 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2342 - val_acc: 0.4375 - val_top_k_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 3.18635\n",
      "Epoch 967/1000\n",
      "\n",
      "Epoch 00967: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7147 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7336 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 3.18635\n",
      "Epoch 968/1000\n",
      "\n",
      "Epoch 00968: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7121 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6589 - val_acc: 0.4875 - val_top_k_categorical_accuracy: 0.7156\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 3.18635\n",
      "Epoch 969/1000\n",
      "\n",
      "Epoch 00969: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7175 - acc: 0.9894 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0541 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6875\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 3.18635\n",
      "Epoch 970/1000\n",
      "\n",
      "Epoch 00970: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7109 - acc: 0.9903 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7809 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 3.18635\n",
      "Epoch 971/1000\n",
      "\n",
      "Epoch 00971: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7104 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.2704 - val_acc: 0.4094 - val_top_k_categorical_accuracy: 0.6406\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 3.18635\n",
      "Epoch 972/1000\n",
      "\n",
      "Epoch 00972: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7186 - acc: 0.9887 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0301 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6594\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 3.18635\n",
      "Epoch 973/1000\n",
      "\n",
      "Epoch 00973: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7122 - acc: 0.9912 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8109 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 3.18635\n",
      "Epoch 974/1000\n",
      "\n",
      "Epoch 00974: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7171 - acc: 0.9903 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7927 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 3.18635\n",
      "Epoch 975/1000\n",
      "\n",
      "Epoch 00975: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7144 - acc: 0.9922 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0990 - val_acc: 0.4250 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 3.18635\n",
      "Epoch 976/1000\n",
      "\n",
      "Epoch 00976: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7103 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.9512 - val_acc: 0.4781 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 3.18635\n",
      "Epoch 977/1000\n",
      "\n",
      "Epoch 00977: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7106 - acc: 0.9900 - top_k_categorical_accuracy: 0.9997 - val_loss: 4.0365 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6844\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 3.18635\n",
      "Epoch 978/1000\n",
      "\n",
      "Epoch 00978: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7142 - acc: 0.9903 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.8768 - val_acc: 0.4562 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 3.18635\n",
      "Epoch 979/1000\n",
      "\n",
      "Epoch 00979: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7162 - acc: 0.9903 - top_k_categorical_accuracy: 0.9981 - val_loss: 3.8517 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 3.18635\n",
      "Epoch 980/1000\n",
      "\n",
      "Epoch 00980: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7105 - acc: 0.9922 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.8419 - val_acc: 0.4625 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 3.18635\n",
      "Epoch 981/1000\n",
      "\n",
      "Epoch 00981: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7093 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8469 - val_acc: 0.5031 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 3.18635\n",
      "Epoch 982/1000\n",
      "\n",
      "Epoch 00982: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7100 - acc: 0.9909 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6975 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7281\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 3.18635\n",
      "Epoch 983/1000\n",
      "\n",
      "Epoch 00983: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7065 - acc: 0.9925 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.1412 - val_acc: 0.4469 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 3.18635\n",
      "Epoch 984/1000\n",
      "\n",
      "Epoch 00984: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7172 - acc: 0.9900 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8988 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6687\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 3.18635\n",
      "Epoch 985/1000\n",
      "\n",
      "Epoch 00985: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7186 - acc: 0.9891 - top_k_categorical_accuracy: 0.9984 - val_loss: 4.3777 - val_acc: 0.4344 - val_top_k_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 3.18635\n",
      "Epoch 986/1000\n",
      "\n",
      "Epoch 00986: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7103 - acc: 0.9906 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8504 - val_acc: 0.4719 - val_top_k_categorical_accuracy: 0.6813\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 3.18635\n",
      "Epoch 987/1000\n",
      "\n",
      "Epoch 00987: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7141 - acc: 0.9884 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7504 - val_acc: 0.4750 - val_top_k_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 3.18635\n",
      "Epoch 988/1000\n",
      "\n",
      "Epoch 00988: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7155 - acc: 0.9909 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.1936 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 3.18635\n",
      "Epoch 989/1000\n",
      "\n",
      "Epoch 00989: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7071 - acc: 0.9931 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7054 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 3.18635\n",
      "Epoch 990/1000\n",
      "\n",
      "Epoch 00990: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7084 - acc: 0.9931 - top_k_categorical_accuracy: 0.9994 - val_loss: 4.0971 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6781\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 3.18635\n",
      "Epoch 991/1000\n",
      "\n",
      "Epoch 00991: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7117 - acc: 0.9928 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.6490 - val_acc: 0.4938 - val_top_k_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 3.18635\n",
      "Epoch 992/1000\n",
      "\n",
      "Epoch 00992: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7085 - acc: 0.9931 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0221 - val_acc: 0.4281 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 3.18635\n",
      "Epoch 993/1000\n",
      "\n",
      "Epoch 00993: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7184 - acc: 0.9906 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.1113 - val_acc: 0.4437 - val_top_k_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 3.18635\n",
      "Epoch 994/1000\n",
      "\n",
      "Epoch 00994: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.7117 - acc: 0.9909 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.5722 - val_acc: 0.5156 - val_top_k_categorical_accuracy: 0.7188\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 3.18635\n",
      "Epoch 995/1000\n",
      "\n",
      "Epoch 00995: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7098 - acc: 0.9912 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.6671 - val_acc: 0.5125 - val_top_k_categorical_accuracy: 0.6937\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 3.18635\n",
      "Epoch 996/1000\n",
      "\n",
      "Epoch 00996: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7055 - acc: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.7621 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 3.18635\n",
      "Epoch 997/1000\n",
      "\n",
      "Epoch 00997: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.7118 - acc: 0.9925 - top_k_categorical_accuracy: 0.9988 - val_loss: 4.0444 - val_acc: 0.4688 - val_top_k_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 3.18635\n",
      "Epoch 998/1000\n",
      "\n",
      "Epoch 00998: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7092 - acc: 0.9912 - top_k_categorical_accuracy: 0.9994 - val_loss: 3.7548 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 3.18635\n",
      "Epoch 999/1000\n",
      "\n",
      "Epoch 00999: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7177 - acc: 0.9900 - top_k_categorical_accuracy: 0.9988 - val_loss: 3.7949 - val_acc: 0.4656 - val_top_k_categorical_accuracy: 0.7094\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 3.18635\n",
      "Epoch 1000/1000\n",
      "\n",
      "Epoch 01000: LearningRateScheduler setting learning rate to 1.0000000000000003e-05.\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.7144 - acc: 0.9906 - top_k_categorical_accuracy: 0.9997 - val_loss: 3.7851 - val_acc: 0.4594 - val_top_k_categorical_accuracy: 0.6906\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 3.18635\n"
     ]
    }
   ],
   "source": [
    "filepath = 'saved_models/caffenet_single_rgb.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=filepath,verbose=1,save_best_only=True)\n",
    "# early_stopper = EarlyStopping(patience=30)\n",
    "# tensorboard = TensorBoard(log_dir='logs')\n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay,verbose=1)\n",
    "\n",
    "\n",
    "model = freeze_all_but_mid_and_top(model)\n",
    "history, model = train_model(model, epochs, training_batches, validation_batches,\n",
    "                             [loss_history,lrate, checkpointer,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05]\n",
      "[5.101588230133057, 4.815394215583801, 4.679335923194885, 4.649224257469177, 4.570973918437958, 4.446535263061524, 4.396056506633759, 4.344961187839508, 4.154906682968139, 4.212078130245208, 4.146889414787292, 4.018550355434417, 3.9937960791587828, 3.957598378658295, 3.8982880449295045, 3.824467182159424, 3.8024027705192567, 3.7209341168403625, 3.7231065154075624, 3.7099719071388244, 3.554990379810333, 3.5430090165138246, 3.5119472551345825, 3.489589147567749, 3.4232108306884768, 3.4501601362228396, 3.3752233672142027, 3.3389111280441286, 3.3006714582443237, 3.2843110013008117, 3.237224452495575, 3.123010928630829, 3.17794647693634, 3.0793975353240968, 3.0640292239189146, 3.0684488129615786, 2.984662125110626, 2.9225174045562743, 2.9547989988327026, 2.8773343753814697, 2.864273226261139, 2.8403684806823732, 2.8803031826019287, 2.779229085445404, 2.7657234025001527, 2.7071924924850466, 2.710082995891571, 2.6538663864135743, 2.623721593618393, 2.6221623611450195, 2.55401380777359, 2.5629379463195803, 2.5339953124523165, 2.5381589138507845, 2.4914872336387632, 2.449361034631729, 2.4795301961898804, 2.465522018671036, 2.4071750354766848, 2.375748952627182, 2.32118612408638, 2.345029377937317, 2.3087904047966004, 2.2798972404003144, 2.2847263157367705, 2.210342284440994, 2.249080231189728, 2.1948549401760102, 2.1857051813602446, 2.178080937862396, 2.1591258883476256, 2.1400864219665525, 2.1492963576316835, 2.0774185073375704, 2.0654448091983797, 2.033692055940628, 2.0106695437431337, 2.0385641145706175, 2.0710084998607634, 1.9813204073905946, 1.975218710899353, 2.0367987787723543, 1.9819722020626067, 1.9553269338607788, 1.9425749957561493, 1.8669632863998413, 1.9257382571697235, 1.8698837423324586, 1.8769815111160277, 1.8436571395397185, 1.8763712859153747, 1.8491924679279328, 1.8343342292308806, 1.8318486499786377, 1.7976529347896575, 1.7510840201377869, 1.7815259790420532, 1.7589308643341064, 1.7807323610782624, 1.763828818798065, 1.7080977940559388, 1.7298007380962372, 1.7297710144519807, 1.70999840259552, 1.7229442262649537, 1.7333891916275024, 1.709027143716812, 1.6733866703510285, 1.688630301952362, 1.6259872508049011, 1.6518746793270112, 1.6620778834819794, 1.6582459688186646, 1.6212787997722626, 1.6479136514663697, 1.5689465034008026, 1.5891503858566285, 1.5956006348133087, 1.5987464427947997, 1.5580078721046449, 1.5519961297512055, 1.5734815502166748, 1.555024938583374, 1.5584963083267211, 1.5349955344200135, 1.5330137622356415, 1.5239899718761445, 1.5019565093517304, 1.52220162153244, 1.5026725494861604, 1.480715960264206, 1.4743219804763794, 1.4864293336868286, 1.507956725358963, 1.4636258745193482, 1.4535799896717072, 1.4719099485874176, 1.4656036794185638, 1.472739145755768, 1.4542746925354004, 1.4439764785766602, 1.4184400296211244, 1.4230844569206238, 1.4076809346675874, 1.432790470123291, 1.415459337234497, 1.3898911356925965, 1.4003806853294372, 1.4179002094268798, 1.3811749041080474, 1.3711295175552367, 1.4052646839618683, 1.3695418417453766, 1.3884171903133393, 1.400880823135376, 1.3324771952629089, 1.3281484043598175, 1.3452047181129456, 1.357538720369339, 1.3577063727378844, 1.351298121213913, 1.3211951088905334, 1.334065328836441, 1.3409458744525908, 1.308933687210083, 1.3535000085830688, 1.322868559360504, 1.2869319748878478, 1.309074237346649, 1.329387674331665, 1.2846545720100402, 1.2909952318668365, 1.2918540668487548, 1.286406408548355, 1.2679121792316437, 1.273602201938629, 1.2831456530094147, 1.251887936592102, 1.2564614355564117, 1.2648282825946808, 1.254795285463333, 1.2756867229938507, 1.256708598136902, 1.26894229888916, 1.2355158674716948, 1.2548253571987151, 1.2295542061328888, 1.2333866965770721, 1.2444004249572753, 1.2275632858276366, 1.2275032442808151, 1.2414698082208633, 1.2149688398838043, 1.2038698786497115, 1.2182716310024262, 1.186189374923706, 1.1991280299425124, 1.1882815718650819, 1.199992458820343, 1.1922448801994323, 1.2040953397750855, 1.1972971028089523, 1.2110767257213593, 1.1802493196725845, 1.201798574924469, 1.147223008275032, 1.1844497847557067, 1.1901675260066986, 1.1430299812555313, 1.2006793677806855, 1.1794578403234481, 1.1983320635557175, 1.1579345321655274, 1.1818990415334703, 1.1685359925031662, 1.128299234509468, 1.1567478108406066, 1.1465523660182952, 1.1218448811769486, 1.1828641021251678, 1.1444622391462327, 1.151167995929718, 1.141505702137947, 1.127005735039711, 1.1198336440324783, 1.1242961966991425, 1.1306086975336074, 1.1230590796470643, 1.136382400393486, 1.154926273226738, 1.1295544928312302, 1.1376903611421585, 1.1214032828807832, 1.1129734355211258, 1.1168067455291748, 1.0902328604459763, 1.1155746120214463, 1.0955161458253861, 1.1075683522224427, 1.094661894440651, 1.1136876171827317, 1.0848953253030778, 1.099659761786461, 1.087747950553894, 1.0909568506479264, 1.0829240173101424, 1.1025934940576554, 1.094025161266327, 1.0614635562896728, 1.086207468509674, 1.0638457107543946, 1.072446547150612, 1.0726908826828003, 1.0844100296497345, 1.0992818260192871, 1.0775412517786025, 1.0892077958583832, 1.040395023226738, 1.0484814649820329, 1.0531229865550995, 1.0652162432670593, 1.0383834290504455, 1.0716784167289735, 1.0620509707927703, 1.0599742406606674, 1.0674333935976028, 1.053850913643837, 1.0489906358718872, 1.0325760853290558, 1.0448542141914368, 1.0267517244815827, 1.038514388203621, 1.0628148466348648, 1.0124887984991073, 1.0169437235593797, 1.0279671144485474, 1.0348936289548873, 1.0386367976665496, 1.026861617565155, 1.0136469674110413, 1.0156037205457686, 1.0120454716682434, 1.0188904452323913, 1.0505481767654419, 1.0453067046403886, 1.0495057386159896, 1.0247200775146483, 1.0133212900161743, 1.0321748351394515, 1.0069068264961243, 1.0126235473155976, 0.9942699080705643, 0.9852090144157409, 0.9892032682895661, 1.0003288805484771, 0.9878208857774734, 0.9893131899833679, 1.0100951671600342, 1.0079441523551942, 0.9782941818237305, 0.9962641406059265, 0.9913063073158264, 0.986136594414711, 0.9962213969230652, 0.9861581522226334, 0.9847091245651245, 0.9809068810939788, 0.9926481682062149, 0.9921665793657303, 0.9863734102249145, 0.9488957017660141, 0.9817219042778015, 0.9903513646125793, 0.957320683002472, 0.963430250287056, 0.9790013122558594, 0.9731566429138183, 0.9598313909769058, 0.9719888669252396, 0.9847873377799988, 0.9671921843290329, 0.9530035120248794, 0.981579994559288, 0.9486389738321305, 0.9531128615140915, 0.9585074019432068, 0.9573636507987976, 0.9275111132860183, 0.9795851814746857, 0.976499627828598, 0.9613661181926727, 0.9710451126098633, 0.9266539359092713, 0.9359479087591172, 0.9382520484924316, 0.9568729382753373, 0.9619453036785126, 0.9492612409591675, 0.9353958010673523, 0.9270256346464157, 0.9273086321353913, 0.9361230331659317, 0.9134156012535095, 0.9325887954235077, 0.9205712628364563, 0.937229984998703, 0.9262905919551849, 0.9402459281682968, 0.929778853058815, 0.9310640174150467, 0.9364834547042846, 0.9423594301939011, 0.9210090827941895, 0.939627532362938, 0.9181683135032653, 0.9283723157644271, 0.9143864220380783, 0.9420823991298676, 0.9171067279577255, 0.9025740772485733, 0.9113638699054718, 0.9148373889923096, 0.9170237481594086, 0.9130642151832581, 0.9163167953491211, 0.9095535236597061, 0.893693699836731, 0.9070565581321717, 0.9184681326150894, 0.914873697757721, 0.9042306542396545, 0.8992863303422928, 0.9033620190620423, 0.9143543016910552, 0.8958246976137161, 0.8904542624950409, 0.9129920095205307, 0.8840001022815704, 0.883200079202652, 0.8866437804698944, 0.8977028006315231, 0.8963920134305954, 0.8889574891328812, 0.8913591027259826, 0.8905613893270492, 0.897134120464325, 0.8850580674409866, 0.8818913978338242, 0.8896249538660049, 0.8811685460805893, 0.8809053915739059, 0.8905648583173752, 0.8698714858293534, 0.8581309133768081, 0.8642559868097305, 0.877049378156662, 0.8593472301959991, 0.8641701292991638, 0.8768347901105881, 0.8665170681476593, 0.8333628040552139, 0.8360065841674804, 0.8312707769870759, 0.8244666004180908, 0.8078596377372742, 0.8124837946891784, 0.8167328292131424, 0.8129516965150834, 0.8087855589389801, 0.8065986782312393, 0.803636412024498, 0.8060734325647354, 0.8043218153715134, 0.8067210966348648, 0.8121501612663269, 0.8011443901062012, 0.8082443976402283, 0.8199779176712036, 0.8003208231925965, 0.8120071846246719, 0.8051599371433258, 0.8033793705701828, 0.802928301692009, 0.7937452185153961, 0.8021118289232254, 0.7973730874061584, 0.7992947280406952, 0.7987647718191146, 0.8058536005020142, 0.8034155070781708, 0.7892067921161652, 0.8031809675693512, 0.7940281760692597, 0.8035000467300415, 0.7876315462589264, 0.7902727764844895, 0.8035512495040894, 0.8025603532791138, 0.7925965076684952, 0.7930335378646851, 0.7953457027673722, 0.7975759261846542, 0.7991930609941482, 0.7992916059494019, 0.7905673271417618, 0.8014956527948379, 0.7932831645011902, 0.7999989330768585, 0.7885075378417968, 0.7952052581310273, 0.7926165133714675, 0.7934830409288406, 0.790101448893547, 0.78543761074543, 0.7875986367464065, 0.7832597827911377, 0.7801948308944702, 0.7890359836816788, 0.8005805003643036, 0.7947019523382187, 0.795913969874382, 0.7903953093290329, 0.7922010868787766, 0.7901385921239853, 0.7894513082504272, 0.7941914802789688, 0.7873835551738739, 0.7906553465127945, 0.7931886601448059, 0.7788379108905792, 0.7838339173793792, 0.7815803956985473, 0.7788930547237396, 0.7946133100986481, 0.784643177986145, 0.7900809454917908, 0.7855154061317444, 0.7896885997056962, 0.7782026368379593, 0.7787580579519272, 0.784285237789154, 0.7766025668382645, 0.796966148018837, 0.779106006026268, 0.776209079027176, 0.7739504951238633, 0.7863511794805527, 0.7719539660215378, 0.7697335040569305, 0.7757501870393753, 0.7813509964942932, 0.7864598953723907, 0.7750755780935288, 0.7771198534965515, 0.7745941680669785, 0.786216094493866, 0.7754446250200272, 0.7835316067934036, 0.7840501195192338, 0.7707727974653245, 0.7812121951580048, 0.7804836696386337, 0.7834685730934143, 0.7766714233160019, 0.7769622933864594, 0.775924088358879, 0.7800905448198319, 0.7711169391870498, 0.7734713691473007, 0.7743951910734177, 0.7727740693092346, 0.7633271837234497, 0.7748244041204453, 0.7788637799024581, 0.7836555498838425, 0.7731651037931442, 0.7746678984165192, 0.7737628245353698, 0.7715776818990707, 0.7712260049581527, 0.775441552400589, 0.7740183997154236, 0.7825973880290985, 0.765179757475853, 0.765869425535202, 0.77544662296772, 0.7748790818452835, 0.7685402518510819, 0.7667806202173233, 0.7657634156942368, 0.7684996712207794, 0.7655537611246109, 0.7640866816043854, 0.7760106605291367, 0.7779874634742737, 0.7754450714588166, 0.7681412929296494, 0.7694676154851914, 0.7691508448123932, 0.7748654603958129, 0.7808649498224258, 0.7719669222831727, 0.7723473328351974, 0.7645354098081589, 0.7747687828540802, 0.7591574823856354, 0.7639301693439484, 0.7609094887971878, 0.7570222461223602, 0.7648728638887405, 0.766545215845108, 0.7644180768728256, 0.7645762711763382, 0.7645290130376816, 0.7662697201967239, 0.7612776654958725, 0.7620503664016723, 0.7645321917533875, 0.7638451510667801, 0.7623562037944793, 0.7627782946825028, 0.7648072439432144, 0.7606294560432434, 0.75878642141819, 0.7651643890142441, 0.7637134325504303, 0.7686159366369247, 0.7496033042669297, 0.764042211174965, 0.7561767345666885, 0.7663415819406509, 0.7759734159708023, 0.7559876352548599, 0.7556792438030243, 0.7654662770032883, 0.7594792544841766, 0.7623118311166763, 0.7570032685995102, 0.7668314886093139, 0.7603258019685746, 0.764006809592247, 0.75484419465065, 0.7580266416072845, 0.766033421754837, 0.758520792722702, 0.7559458196163178, 0.7576888501644135, 0.76414168725582, 0.7536508131027222, 0.759643565416336, 0.7542961817979813, 0.7515191304683685, 0.7565783345699311, 0.7449730724096298, 0.7554226750135422, 0.7508908373117447, 0.7601044899225236, 0.7524660497903823, 0.7448853868246078, 0.7538934445381165, 0.757082547545433, 0.7534822100400924, 0.7570655941963196, 0.7570056462287903, 0.7519859892129898, 0.7460576164722442, 0.7515502887964248, 0.7587405800819397, 0.7504019886255264, 0.7488804346323014, 0.7457634925842285, 0.7539042592048645, 0.7476735365390778, 0.754986315369606, 0.7510199332237244, 0.7540828919410706, 0.7484890586137771, 0.754887797832489, 0.7447857052087784, 0.7530565321445465, 0.7408552277088165, 0.7434961080551148, 0.7496523994207382, 0.7440793317556381, 0.748439222574234, 0.7515096348524094, 0.7506557542085648, 0.7567703175544739, 0.7564477956295014, 0.748223807811737, 0.7444831252098083, 0.7564011353254319, 0.7562986463308334, 0.7516537284851075, 0.7541071587800979, 0.7497032046318054, 0.7466792517900467, 0.7547873878479003, 0.75115593791008, 0.7381776964664459, 0.7378580141067504, 0.745778751373291, 0.7506230336427688, 0.7423377865552903, 0.7440788996219635, 0.742072365283966, 0.7341745859384536, 0.74436115026474, 0.7406805533170701, 0.741406638622284, 0.7377693629264832, 0.7444389969110489, 0.7387489950656891, 0.7429107141494751, 0.7460333043336869, 0.744493481516838, 0.7428371131420135, 0.7453067857027054, 0.7406674683094024, 0.7439475339651108, 0.7472349417209625, 0.7483732050657272, 0.7412418192625045, 0.7398394501209259, 0.743324625492096, 0.7431146210432052, 0.7446940249204635, 0.7432338684797287, 0.7389761984348298, 0.7381021535396576, 0.7432055616378784, 0.7435509657859802, 0.7453655636310578, 0.7410052406787873, 0.7414863097667694, 0.7430309772491455, 0.7354054808616638, 0.7419971245527267, 0.7410630059242248, 0.7450770181417465, 0.7439335638284683, 0.748467025756836, 0.7353525221347809, 0.74128158390522, 0.7345789557695389, 0.7332151001691818, 0.7352924805879593, 0.7447323101758957, 0.7350761550664902, 0.7403105717897415, 0.7339397203922272, 0.7342805743217469, 0.733300331234932, 0.736515970826149, 0.741972696185112, 0.7324878215789795, 0.7300116968154907, 0.7378266477584838, 0.7375903022289276, 0.7383448708057404, 0.7415239477157592, 0.7439015418291092, 0.7354076433181763, 0.7367326498031617, 0.7335190904140473, 0.7385841339826584, 0.7319869667291641, 0.7292612195014954, 0.7324790012836456, 0.7332292711734771, 0.7337368988990783, 0.7436617982387542, 0.7284377390146255, 0.746460639834404, 0.7232493084669113, 0.7346854215860367, 0.7396796363592147, 0.7352831155061722, 0.7438384902477264, 0.7338618642091751, 0.7389206600189209, 0.7349416738748551, 0.7352992677688599, 0.7403065407276154, 0.7407215732336044, 0.7406261509656906, 0.732392994761467, 0.7304492580890656, 0.7373310512304306, 0.7342158305644989, 0.7256802994012833, 0.7329377973079682, 0.7290151333808899, 0.7224349349737167, 0.7352067196369171, 0.7251227086782456, 0.724295819401741, 0.7202765375375748, 0.739195984005928, 0.7288637346029282, 0.7259757989645004, 0.7237220269441604, 0.7361383801698684, 0.7270879995822906, 0.7294526046514511, 0.7265756434202194, 0.7335140919685363, 0.7269006913900375, 0.7225394713878631, 0.7330007010698318, 0.7286177402734757, 0.7294424450397492, 0.7311635082960128, 0.7224054908752442, 0.7212938749790192, 0.7233547085523605, 0.7214315646886825, 0.7250952136516571, 0.7268149995803833, 0.7284342789649964, 0.724351515173912, 0.7287075626850128, 0.7280904388427735, 0.7277125239372253, 0.7356431698799133, 0.7275286757946015, 0.7202655071020126, 0.72153768658638, 0.723618084192276, 0.7199147886037827, 0.7215745288133621, 0.7268372428417206, 0.7161499315500259, 0.7187997591495514, 0.7183741211891175, 0.7181426566839219, 0.7340482819080353, 0.7173991358280182, 0.7261248809099198, 0.719307251572609, 0.7242884439229965, 0.7196903651952744, 0.7134517270326615, 0.7230789989233017, 0.7305869770050049, 0.7217343032360077, 0.7265893125534058, 0.7340487068891526, 0.7309705930948257, 0.7244186359643936, 0.7234275633096695, 0.7236251443624496, 0.7160174483060837, 0.7206518381834031, 0.7135441040992737, 0.7172669726610184, 0.7160850423574447, 0.7100310218334198, 0.7153304445743561, 0.7142006212472916, 0.7216537684202194, 0.7148155975341797, 0.7231284898519516, 0.7235919165611268, 0.7213806617259979, 0.7162003022432327, 0.7127326619625092, 0.721402987241745, 0.7195166170597076, 0.7134092491865158, 0.713072789311409, 0.7169783145189286, 0.7156734764575958, 0.717258768081665, 0.7153057879209519, 0.7106902849674225, 0.7154321610927582, 0.7165840339660644, 0.7090849405527115, 0.7226501095294953, 0.7132804405689239, 0.7142236346006393, 0.7149446070194244, 0.7144132000207901, 0.7170597034692764, 0.7187581771612167, 0.7141984909772873, 0.7244137275218964, 0.7066850805282593, 0.7185284161567688, 0.7204571014642716, 0.7151236826181412, 0.7169922316074371, 0.716059780716896, 0.722832545042038, 0.7135039728879928, 0.7114005345106125, 0.7105959224700927, 0.7160336321592331, 0.715609354376793, 0.7146997517347335, 0.7152754634618759, 0.715192785859108, 0.7130137324333191, 0.7139351683855056, 0.7116785591840744, 0.7124678230285645, 0.71385089635849, 0.7143152606487274, 0.7237450110912323, 0.722380222082138, 0.721876962184906, 0.7159008914232254, 0.7063703387975693, 0.7191575878858566, 0.7241361087560654, 0.7177662539482117, 0.710146763920784, 0.7168742549419403, 0.7137527114152908, 0.7178876090049744, 0.7172641634941102, 0.7098831516504288, 0.7150688779354095, 0.7147327899932862, 0.7193668687343597, 0.709448094367981, 0.7164355087280273, 0.712146412730217, 0.7091553890705109, 0.7168206173181534, 0.7128356683254242, 0.7093490356206894, 0.7097677940130234, 0.7180243700742721, 0.7117218667268753, 0.713003346323967, 0.7150557208061218, 0.7194713580608368, 0.7184471529722214, 0.7130403161048889, 0.7169524878263474, 0.7123663169145584, 0.7187097615003586, 0.7163851433992385, 0.7108586728572845, 0.7082676270770997, 0.7163429814577102, 0.7169761228561401, 0.7127763104438781, 0.7129590982198715, 0.7111985778808594, 0.7125688982009888, 0.70832883477211, 0.7154784458875656, 0.7122161877155304, 0.7196565145254135, 0.7102605164051056, 0.7181119537353515, 0.7110265094041824, 0.7247838342189789, 0.711646459698677, 0.7140687292814255, 0.7052960729598999, 0.7082952934503556, 0.7167880362272263, 0.712747005224228, 0.7098044401407242, 0.7103106725215912, 0.7053911817073822, 0.7194040209054947, 0.7164965921640396, 0.7176366013288498, 0.7192273133993149, 0.7125977051258087, 0.711449436545372, 0.7186528074741364, 0.7129456949234009, 0.7070886635780335, 0.7122648429870605, 0.7097850823402405, 0.7187111961841584, 0.7110347527265549, 0.7155616420507431, 0.7159945285320282, 0.7127252453565598, 0.7106436312198638, 0.7116303443908691, 0.718633239865303, 0.7068636387586593, 0.71326669216156, 0.7161961388587952, 0.7129216921329499, 0.7115962213277817, 0.7146952795982361, 0.7116180849075318, 0.7095944160223007, 0.7050341027975082, 0.7132495927810669, 0.7113664942979813, 0.7145296961069107, 0.7143324321508407, 0.7146040165424347, 0.7151630914211273, 0.711305330991745, 0.7058127337694168, 0.7136240845918655, 0.7179294317960739, 0.7117444217205048, 0.7135488468408585, 0.7107680732011795, 0.7159282755851746, 0.7104792529344559, 0.7172727239131927, 0.7130451357364654, 0.7149182403087616, 0.7093580383062362, 0.7129865360260009, 0.7179764050245285, 0.7109141874313355, 0.7100964480638504, 0.7207353752851486, 0.7157630890607833, 0.7109492552280426, 0.7156438684463501, 0.7123921543359757, 0.7146884548664093, 0.7120804423093796, 0.7174972230196, 0.7108605492115021, 0.7103783452510833, 0.7185581612586975, 0.7122311305999756, 0.7170512706041337, 0.7143781590461731, 0.7102942246198655, 0.7105513459444046, 0.7141747337579727, 0.7162390571832656, 0.7105037778615951, 0.7092605781555176, 0.7099780410528183, 0.7065355849266052, 0.7171899646520614, 0.7185733187198639, 0.7102624136209488, 0.7140616339445114, 0.7154837727546692, 0.7071428149938583, 0.7084411591291427, 0.7116670835018158, 0.7085262614488602, 0.7184370291233063, 0.7116955888271331, 0.7098367697000504, 0.7054892128705978, 0.7118076223134995, 0.7091698127985001, 0.7176814943552017, 0.7143929570913314]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history.lr)\n",
    "print(loss_history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16904/16904 [==============================] - 1422s 84ms/step\n",
      "[3.883246474690873, 0.46692350922858494, 0.7175002957879791]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(generator=testing_batches,verbose=1,steps=testing_batches2.samples//batch_size)\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
